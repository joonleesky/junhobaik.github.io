---
title:  "(작성중) [CS 285] Lecture 8: Deep RL with Q-Functions"
date:   2020-02-25 12:04:36 +0900
tags:
  - CS285
---
이번 강의에선 deep learning과 다양한 trick들을 이용해서 어떻게 현실의 문제들에 Q-learning을 적용할 수 있는지 살펴보도록 하겠습니다. CS 285 강의를 기본으로 하되 흐름에 맞게 내용들을 추가하고 재배치했음을 밝힙니다. 부족한점에 대한 지적과 질문은 자유롭게 댓글로 남겨주세요.

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

## Deep Q-network

지난 시간에는 Q-value를 학습하여 좋은 policy를 찾아나가는 Q-learning 알고리즘을 살펴보았습니다.

<center><img src = "/assets/images/cs285/lecture7/qlearn.png" width = "450"></center><br>

여기서 function approximator $$\phi$$로 deep neural network를 사용하는 알고리즘을 **DQN(Deep Q-network)**이라고 부릅니다. 하지만 DQN 알고리즘은 학습 과정이 너무나 unstable해서 $$Q_{\phi}(s,a)$$를 수렴시키기가 어렵고, 수렴하더라도 local-optima에 쉽게 빠지곤 합니다. 
이번 강의에선 DQN 알고리즘이 왜 그러한 문제점들을 겪는지, 문제점들을 해결할 수 있는 해결책들은 어떠한 것들이 있는지 하나씩 살펴보도록 하겠습니다.

### Replay Buffer

첫번째로 살펴볼 문제는 **Correlated samples**입니다. DQN 알고리즘에서는 simulation을 통해서 data를 모으는 과정과, model을 학습하는 과정이 거의 동시에 일어나게 됩니다. 
하지만 하나의 trajectory에서 획득한 sample data들은 상당히 유사하기 때문에 sample들의 correlation역시 높을 수 밖에 없습니다. 
중간의 그림과 같이 저희의 sample들의 correlation이 높다면, Q-function역시 특정 sample들에만 최적화되어 학습이 진행됩니다. 

<center><img src = "/assets/images/cs285/lecture8/corr_samples.png" width = "800"></center><br>

따라서, 좋은 DQN을 잘 학습시키기 위해서는 correlation이 낮은 sample들을 모으는 것이 중요합니다. 단순한 방법중 하나는 학습을 parallel하게 진행하여 하나의 trajectory가 아닌 여러개의 trajectory에서 sample들을 동시에 수집하는 것입니다.

<center><img src = "/assets/images/cs285/lecture8/parallel_dqn.png" width = "600"></center><br>

사실 7강에서 살펴봤던 Q-learning의 가장 큰 장점중 하나는 학습이 off-policy로 이루어질 수 있다는 것입니다. 
$$ Q(s,a) $$를 학습하는 과정은 현재의 policy $$ \pi $$와 전혀 무관하기 때문에 어떠한 policy에서 생성된 data도 학습에 활용할 수 있습니다.
따라서, 과거의 sample들을 저장해둘 Replay Buffer $$\mathcal{B}$$를 생성하고 $$\mathcal{B}$$에서 data들을 sampling하며 학습을 진행하면 sample들의 correlation을 현저하게 낮출 수 있게 됩니다.

### Target Network

Replay buffer를 이용하여 sample의 correlation문제는 해결하였지만, 여전히 DQN의 학습 과정은 너무나 unstable합니다. 
Unstable training의 가장 큰 원인 중 하나는, 학습하고자 하는 target이 움직인다는 것입니다.

<center><img src = "/assets/images/cs285/lecture8/mv_target.png" width = "600"></center><br>

저희의 target $$y_i$$역시 $$Q_{\phi}$$로 parameterized된 값이기 때문에, $$Q_{\phi}(s_i, a_i)$$가 $$y_i$$가 되도록 regression을 진행해면 $$y_i$$의 값 역시 변하게 됩니다.
이런 **moving target** 문제를 해결하기 위해 deepmind에서 제시한 방법은 target network, $$Q_{phi^{\prime}}$$을 사용하는 것입니다.
Target network를 사용한다면, $$\phi$$가 update되더라도 target $$y_i$$는 $$\phi^{\prime}$$를 통해 정해지기 때문에 target의 위치가 고정되어있게 됩니다.

그렇다면 target network는 어떻게 update를 할까요? 정해둔 주기마다 $$\phi^{\prime}$$을 $$\phi$$의 파라미터로 대체하는 것입니다.

**Replay Buffer**와 **Target Network**를 사용한 DQN 알고리즘을 종합하면 아래와 같습니다.

<center><img src = "/assets/images/cs285/lecture8/dqn.png" width = "800"></center><br>

2015년 deepmind가 발표한 DQN 알고리즘은 실제로 다양한 atari game 환경에서 인간보다도 뛰어난 성능을 보여주며 큰 주목을 받게 됩니다. [Mnih et al][dqn-2015]

<center><img src = "/assets/images/cs285/lecture8/dqn_results.png" width = "500"></center><br>

### Double Q-learning

비록 DQN 알고리즘은 다양한 game에서 높은 성능을 보여주었지만, 실제 agent에 성능에 비해 Q-network의 값이 훨씬 over-estimated된 결과를 보여주었습니다.

<center><img src = "/assets/images/cs285/lecture8/double.png" width = "650"></center><br>

Q-learning에서 value function이 over-estimate된 결과를 예측하는 현상을 **Maximization Bias**라고 말합니다. 
Maximization Bias가 발생하는 이유를 아래의 예시를 통해 살펴보도록 하겠습니다.

<center><img src = "/assets/images/cs285/lecture8/maxbias.png" width = "450"></center><br>

Agent가 현재 state A에 놓여있고 왼쪽과 오른쪽 중에 선택을 해야하는 상황에 놓여있는 상황을 가정해 보겠습니다.  <br>
Agent가 오른쪽을 선택하면 0의 reward를 받고 게임이 종료되며, 왼쪽을 선택하면 0의 reward를 받고 state B에 도착합니다. <br>
State B에서는 어떠한 action을 취하든 $$ \mathcal{N}(-0.1, 1) $$의 reward를 받고 게임이 종료됩니다. <br>
State B에서는 action에 상관없이 언제나 음의 기댓값을 가지고 있기 때문에, agent의 optimal action은 당연히 오른쪽 행동을 선택하는 것이 됩니다.
그렇다면, 간단한 예시와 함께 agent가 정말로 오른쪽 행동을 선택하게 되는지 확인해보도록 하겠습니다.

저희의 agent는 state B에서 두개의 다른 action a1, a2를 3번씩 경험하며 아래와 같이 Q값을 update해 나아갑니다.

$$ Q(s, a) \leftarrow 0.5 * Q(s, a) + 0.5 * (r(s, a) + V(s^{\prime})) $$

| $$Q(A,left)$$ | $$r(B,a1)$$ | $$r(B,a2)$$ | $$Q(B, a1)$$ | $$Q(B, a2)$$ |$$max_a Q(B,a)$$  |
|:-------------:|:-----------:|:-----------:|:------------:|:------------:|:----------------:|
| $$0 $$        | $$-0.4$$    |  $$0.2$$    | $$ -0.2$$    |   $$0.1$$    |    $$0.1$$       |
|  $$0.05$$     | $$-0.2$$    |  $$0.2$$    |   $$ -0.2 $$ |   $$0.2$$    |    $$0.2$$       |
|  $$0.125$$    | $$ 0$$      |  $$-0.6$$   |  $$-0.1$$    |   $$-0.2$$   |    $$-0.1$$      |
|  $$0.0125$$   |

<br>

위의 table에서 보듯이, $$Q(B,a1)$$과 $$Q(B,a2)$$가 음의 값을 가지고 있음에도 불구하고 $$Q(A,left)$$는 양의 값으로 학습되는 현상이 나타나게 되었습니다. 
이런 문제가 나타나는 근본적인 원인은 $$V(s^{\prime})$$을 추정하기 위해 $$max_a Q(s^{\prime},a)$$을 사용하기 때문입니다. 
학습중인 $$Q(s,a)$$는 완벽하지 않기 때문에 noise가 존재하게 되는데 $$max$$ 연산으로 인하여 큰 값의 noise가 bias처럼 섞여 들어가게 되어 $$Q(s,a)$$를 overestimate하게 되는 현상이 발생하게 됩니다.

이런 문제를 해결하는 간단한 방법은, 다음 state의 action을 선택하는 함수와 값을 계산하는 함수를 분리하는 것입니다.

$$ Q_{B}(s,a) \leftarrow r + \gamma Q_{A}(s^{\prime}, arg\, max_{a} Q_{B}(s^{\prime},a)) $$

이렇게 하면 noise에 대한 bias가 없어지게 되어 over-estimation 문제를 해결할 수 있습니다. 
그렇다면, 이러한 Double Q-learning strategy를 저희의 DQN에 어떻게 효과적으로 적용할 수 있을까요?
Policy network $$Q_{phi}$$와 Targe network $$Q_{phi^{\prime}}$$을 2개의 Q-network로 사용하는 것입니다. 

$$ y \leftarrow r + \gamma Q_{\phi^{\prime}}(s^{\prime}, arg\, max_{a} Q_{\phi^{\prime}}(s^{\prime},a)) $$

위처럼 생성되던 DQN의 target에서, 다음 state의 action selection 부분을 $$ Q_{\phi}$$로 대체합니다.

$$ y \leftarrow r + \gamma Q_{\phi^{\prime}}(s^{\prime}, arg\, max_{a} Q_{\phi}(s^{\prime},a)) $$

비록, $$Q_{\phi^{\prime}}$$는 $$Q_{\phi}$$의 파라미터로 주기적으로 초기화 해주기 때문에 문제가 해결됬다고 말할 순 없지만, 
이전에 DQN에서 moving target을 위해 생성했던 target network를 효과적으로 활용하여 over-estimation문제 역시 크게 개선할 수 있습니다.

### Dueling architecture



### Multi-step returns

## Q-learning with continuous action spaces

### NAF: Normalized Advantage Function

### DDPG: Deep Deterministic Policy Gradient

## Summing up

<br>

## Related Papers
* (DQN) Playing Atari with Deep Reinforcement Learning. Mnih et al. NIPS 2015.
* (DQN) Human-level control through deep reinforcement learning. Mnih et al. Nature 2015.
* (Double DQN) Deep Reinforcement Learning with Double Q-learning. Hasselt et al. AAAI 2016.
* (Dueling DQN) Dueling Network Arhictectures for Deep Reinforcement Learning. Wang et al. ICML 2016.
* (DDPG) Continuous control with Deep Reinforcement Learning. Lillicrap et al. ICLR 2016.
* (NAF) Continuous Deep Q-Learning with Model-based Acceleration. Shixiang Gu et al. arxiv 2016.



[dqn-2015]: https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning