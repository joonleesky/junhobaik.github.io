---
title:  "[CS 285] Lecture 8: Deep RL with Q-Functions"
date:   2020-02-25 12:04:36 +0900
tags:
  - CS285
---
이번 강의에선 deep learning과 다양한 trick들을 이용해서 어떻게 현실의 문제들에 Q-learning을 적용할 수 있는지 살펴보도록 하겠습니다. CS 285 강의를 기본으로 하되 흐름에 맞게 내용들을 추가하고 재배치했음을 밝힙니다. 부족한점에 대한 지적과 질문은 자유롭게 댓글로 남겨주세요.

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

## Deep Q-network

지난 시간에는 Q-value를 학습하여 좋은 policy를 찾아나가는 Q-learning 알고리즘을 살펴보았습니다.

<center><img src = "/assets/images/cs285/lecture7/qlearn.png" width = "400"></center><br>

여기서 function approximator $$\phi$$로 deep neural network를 사용하는 알고리즘을 **DQN(Deep Q-network)**이라고 부릅니다. 하지만 DQN 알고리즘은 학습 과정이 너무나 unstable해서 $$Q_{\phi}(s,a)$$를 수렴시키기가 어렵고, 수렴하더라도 local-optima에 쉽게 빠지곤 합니다. 
이번 강의에선 DQN 알고리즘이 왜 그러한 문제점들을 겪는지, 문제점들을 해결할 수 있는 해결책들은 어떠한 것들이 있는지 하나씩 살펴보도록 하겠습니다.

### Replay Buffer

첫번째로 살펴볼 문제는 **Correlated samples**입니다. DQN 알고리즘에서는 simulation을 통해서 data를 모으는 과정과, model을 학습하는 과정이 거의 동시에 일어나게 됩니다. 
하지만 하나의 trajectory에서 획득한 sample data들은 상당히 유사하기 때문에 sample들의 correlation역시 높을 수 밖에 없습니다. 
중간의 그림과 같이 저희의 sample들의 correlation이 높다면, Q-function역시 특정 sample들에만 최적화되어 학습이 진행됩니다. 

<center><img src = "/assets/images/cs285/lecture8/corr_samples.png" width = "800"></center><br>

따라서, 좋은 DQN을 잘 학습시키기 위해서는 correlation이 낮은 sample들을 모으는 것이 중요합니다. 단순한 방법중 하나는 학습을 parallel하게 진행하여 하나의 trajectory가 아닌 여러개의 trajectory에서 sample들을 동시에 수집하는 것입니다.

<center><img src = "/assets/images/cs285/lecture8/parallel_dqn.png" width = "600"></center><br>

사실 7강에서 살펴봤던 Q-learning의 가장 큰 장점중 하나는 학습이 off-policy로 이루어질 수 있다는 것입니다. 
$$ Q(s,a) $$를 학습하는 과정은 현재의 policy $$ \pi $$와 전혀 무관하기 때문에 어떠한 policy에서 생성된 data도 학습에 활용할 수 있습니다.
따라서, 과거의 sample들을 저장해둘 Replay Buffer $$\mathcal{B}$$를 생성하고 $$\mathcal{B}$$에서 data들을 sampling하며 학습을 진행하면 sample들의 correlation을 현저하게 낮출 수 있게 됩니다.

### Target Network

Replay buffer를 이용하여 sample의 correlation문제는 해결하였지만, 여전히 DQN의 학습 과정은 너무나 unstable합니다. 
Unstable training의 가장 큰 원인 중 하나는, 학습하고자 하는 target이 움직인다는 것입니다.

<center><img src = "/assets/images/cs285/lecture8/mv_target.png" width = "600"></center><br>

저희의 target $$y_i$$역시 $$Q_{\phi}$$로 parameterized된 값이기 때문에, $$Q_{\phi}(s_i, a_i)$$가 $$y_i$$가 되도록 regression을 진행해면 $$y_i$$의 값 역시 변하게 됩니다.
이런 **moving target** 문제를 해결하기 위해 deepmind에서 제시한 방법은 target network, $$Q_{phi^{\prime}}$$을 사용하는 것입니다.
Target network를 사용한다면, $$\phi$$가 update되더라도 target $$y_i$$는 $$\phi^{\prime}$$를 통해 정해지기 때문에 target의 위치가 고정되어있게 됩니다.

그렇다면 target network는 어떻게 update를 할까요? 정해둔 주기마다 $$\phi^{\prime}$$을 $$\phi$$의 파라미터로 대체하는 것입니다.

**Replay Buffer**와 **Target Network**를 사용한 DQN 알고리즘을 종합하면 아래와 같습니다.

<center><img src = "/assets/images/cs285/lecture8/dqn.png" width = "800"></center><br>

2015년 deepmind가 발표한 DQN 알고리즘은 실제로 다양한 atari game 환경에서 인간보다도 뛰어난 성능을 보여주며 큰 주목을 받게 됩니다. [Mnih et al][dqn-2015]

<center><img src = "/assets/images/cs285/lecture8/dqn_results.png" width = "500"></center><br>

### Double Q-learning

비록 DQN 알고리즘은 다양한 game에서 높은 성능을 보여주었지만, 실제 agent에 성능에 비해 Q-network의 값이 훨씬 over-estimated된 결과를 보여주었습니다.

<center><img src = "/assets/images/cs285/lecture8/double.png" width = "650"></center><br>

Q-learning에서 value function이 over-estimate된 결과를 예측하는 현상을 **Maximization Bias**라고 말합니다. 
Maximization Bias가 발생하는 이유를 아래의 예시를 통해 살펴보도록 하겠습니다.

<center><img src = "/assets/images/cs285/lecture8/maxbias.png" width = "450"></center><br>

Agent가 현재 state A에 놓여있고 왼쪽과 오른쪽 중에 선택을 해야하는 상황에 놓여있는 상황을 가정해 보겠습니다.  <br>
Agent가 오른쪽을 선택하면 0의 reward를 받고 게임이 종료되며, 왼쪽을 선택하면 0의 reward를 받고 state B에 도착합니다. <br>
State B에서는 어떠한 action을 취하든 $$ \mathcal{N}(-0.1, 1) $$의 reward를 받고 게임이 종료됩니다. <br>
State B에서는 action에 상관없이 언제나 음의 기댓값을 가지고 있기 때문에, agent의 optimal action은 당연히 오른쪽 행동을 선택하는 것이 됩니다.
그렇다면, 간단한 예시와 함께 agent가 정말로 오른쪽 행동을 선택하게 되는지 확인해보도록 하겠습니다.

저희의 agent는 state B에서 두개의 다른 action a1, a2를 3번씩 경험하며 아래와 같이 Q값을 update해 나아갑니다.

$$ Q(s, a) \leftarrow 0.5 * Q(s, a) + 0.5 * (r(s, a) + V(s^{\prime})) $$

| $$Q(A,left)$$ | $$r(B,a1)$$ | $$r(B,a2)$$ | $$Q(B, a1)$$ | $$Q(B, a2)$$ |$$max_a Q(B,a)$$  |
|:-------------:|:-----------:|:-----------:|:------------:|:------------:|:----------------:|
| $$0 $$        | $$-0.4$$    |  $$0.2$$    | $$ -0.2$$    |   $$0.1$$    |    $$0.1$$       |
|  $$0.05$$     | $$-0.2$$    |  $$0.2$$    |   $$ -0.2 $$ |   $$0.2$$    |    $$0.2$$       |
|  $$0.125$$    | $$ 0$$      |  $$-0.6$$   |  $$-0.1$$    |   $$-0.2$$   |    $$-0.1$$      |
|  $$0.0125$$   |

<br>

위의 table에서 보듯이, $$Q(B,a1)$$과 $$Q(B,a2)$$가 음의 값을 가지고 있음에도 불구하고 $$Q(A,left)$$는 양의 값으로 학습되는 현상이 나타나게 되었습니다. 
이런 문제가 나타나는 근본적인 원인은 $$V(s^{\prime})$$을 추정하기 위해 $$max_a Q(s^{\prime},a)$$을 사용하기 때문입니다. 
학습중인 $$Q(s,a)$$는 완벽하지 않기 때문에 noise가 존재하게 되는데 $$max$$ 연산으로 인하여 큰 값의 noise가 bias처럼 섞여 들어가게 되어 $$Q(s,a)$$를 overestimate하게 되는 현상이 발생하게 됩니다.

이런 문제를 해결하는 간단한 방법은, 다음 state의 action을 선택하는 함수와 값을 계산하는 함수를 분리하는 것입니다.

$$ Q_{B}(s,a) \leftarrow r + \gamma Q_{A}(s^{\prime}, arg\, max_{a} Q_{B}(s^{\prime},a)) $$

이렇게 하면 noise에 대한 bias가 없어지게 되어 over-estimation 문제를 해결할 수 있습니다. 
그렇다면, 이러한 Double Q-learning의 strategy를 저희의 DQN에 어떻게 효과적으로 적용할 수 있을까요?
바로 policy network $$Q_{\phi}$$와 targe network $$Q_{\phi^{\prime}}$$을 2개의 Q-network로 사용하는 것입니다.
기존의 neural network의 target을 다시 살펴보겠습니다.

$$ y \leftarrow r + \gamma Q_{\phi^{\prime}}(s^{\prime}, arg\, max_{a} Q_{\phi^{\prime}}(s^{\prime},a)) $$

Target은 고정시킨채 action selection 과정을 독립시킥 위하여, 다음 state의 action selection을 $$ Q_{\phi}$$로 대체합니다.

$$ y \leftarrow r + \gamma Q_{\phi^{\prime}}(s^{\prime}, arg\, max_{a} Q_{\phi}(s^{\prime},a)) $$

비록, $$Q_{\phi^{\prime}}$$는 $$Q_{\phi}$$의 파라미터로 주기적으로 초기화 해주기 때문에 noise에 대한 bias를 없앨 수는 없지만, 
겨우 1줄 남짓의 코드 변화를 통해서 over-estimation문제를 크게 개선할 수 있습니다.

### Dueling architecture

2016년 Deepmind팀은 DQN의 architecture를 dueling형태로 바꾸여 또 다시 atari domain에서 SOTA를 갱신하였습니다. 

Q-value가 value와 advantage의 결합으로 나타낼 수 있다는 점에서 착안하여, dueling network를 제안하였습니다.
$$Q(s,a)$$를 직접적으로 구하는 것이 아니라 value와 advantage를 학습하여, 간접적으로 값을 구하는 것이지요. 

<center><img src = "/assets/images/cs285/lecture8/dueling_architect.png" width = "450"></center><br>

그렇다면 이렇게 $$Q(s,a)$$를 간접적으로 학습하는 것이 성능 향상에 왜 도움을 주었을까요?
Atari 게임 중 장애물을 피하는 enduro 게임을 한번 살펴보겠습니다.

<center><img src = "/assets/images/cs285/lecture8/enduro.gif" width = "300" style = "margin-bottom:12px"><img src = "/assets/images/cs285/lecture8/dueling_result.png" width = "350"></center>
<center>Gameplay of Enduro</center><div style = "margin-bottom:12px"></div>

왼쪽 동영상에서 보듯이 실제로 action의 선택이 중요한 경우는 오직 장애물이 눈 앞으로 다가온 경우입니다. 
장애물이 보이지 않거나 상당히 먼 거리에 있다면, 어떠한 action을 취해도 상관이 없기 때문에 action에 상관없이 같은 Q-value를 가져야 하겠지요.

Dueling architecture는 action에 상관없이 state의 value를 효과적으로 학습할 수 있습니다. 
오른쪽 그림은 state별로 생성된 gradient의 saliency map입니다. 
Value network는 현재 상황의 가치를 측정하기 위해 아래의 점수판과 멀리서 다가오는 장애물에 집중하고 있는 반면, advantage network는 action간의 가치의 차이에 집중하기 위해 가까이 다가온 장애물에 집중하고 있는 것입니다.
이와 같이 각각의 network가 효과적으로 attention을 가질 수 있게 되어 학습이 효율적으로 이루어지게 됩니다.

실제 구현에 있어서는 한 가지 주의해야할 문제점이 있는데, 바로 **identifibility**입니다.
$$Q$$값이 주어진채로 $$V$$와 $$A$$를 학습할 때, 두 값의 조합은 unique하지 않기 때문에 $$V$$와 $$A$$가 각각 어떤 값으로 학습되어야 하는지가 명확하지가 않습니다.

따라서, $$Q(s,a^*) = V(s),\,\, A(s, a^*) = 0$$이라는 점에서 착안하여 $$Q(s,a)$$의 값을 아래와 같이 계산할 수 있습니다.

$$Q(s,a) = V(s) + (A(s,a) - arg max_{a^{\prime}}(A(s,a^{\prime})))$$

하지만, 위의 방식은 maximum advantage action을 선택하지 않은 경우에 $$Q$$값의 stability가 크게 떨어질 수 있습니다. 
따라서 max operator를 average로 바꾸는 방법을 생각해볼 수 있습니다.

$$Q(s,a) = V(s) + (A(s,a) - \cfrac{1}{\vert\mathcal{A} \vert} \sum_{a^{\prime}}(A(s,a^{\prime})))$$

비록 평균을 빼주는 수식은 실제 $$V$$와 $$A$$의 의미에서 벗어나게 하지만 action간의 ranking은 여전히 보존시키며 stability를 가져올 수 있습니다.

### Multi-step returns

이전에 actor-critic 알고리즘에서 살펴봤던 n-step return을 돌아보도록 하겠습니다.

High variance, Low bias: $$\,\,\,\,\,\, R = \sum^{T}_{t^\prime=t} \gamma^{t^{\prime} - t} r(s_{t^{\prime}},a_{t^{\prime}}) $$
<div style = "margin-bottom:14px"></div>
Low variance, High bias: $$\,\,\,\,\,\, R = r(s_t,a_t) + \gamma \, V(s_{t+1})$$
<div style = "margin-bottom:14px"></div>
Medium variance, Medium bias: $$\,\,\,\,\,\, R = \sum^{t^\prime=t+N-1}_{t^\prime=t} \gamma^{t^{\prime} - t} r(s_{t^{\prime}},a_{t^{\prime}}) + \gamma^{N}V(s_{t+n}) $$

현재 DQN의 target은 low variance, high bias 방법으로 생성되고 있으니 n-step return을 통해서 bias를 줄여주는 것입니다.

$$ y \leftarrow \sum^{t^\prime=t+N-1}_{t^\prime=t} \gamma^{t^{\prime} - t} r(s_{t^{\prime}},a_{t^{\prime}}) + \gamma^{N} max_{a_{t+N}}Q_{\phi^{\prime}}(s_{t+N},a_{t+N})) $$

하지만, N-step return은 $$r_{t+1}, r_{t+2}, ... r_{t+N-1}$$이 사용됨으로 on-policy에서 사용할 때만 올바른 target을 생성하게 됩니다.

N-step return을 DQN에 적용하기 위한 방법은 어떤 것들이 있을까요?

1. Train with on-policy manner
2. Importance Sampling
3. Ignore the problem

사실, $$\pi$$가 크게 변하지 않는다면 target이 크게 다르지 않은 값으로 생성되기 때문에 3번과 같이 문제를 무시해버리더라도 bias를 낮춰 학습이 효율적으로 이루어지는 경우가 상당히 많습니다.

### To the Rainbow

지금까지 Deep Q-network를 개선하기 위한 정말 다양한 해결책들을 살펴보았습니다.
여기서, 과연 이러한 해결책들을 함께 사용해도 부작용이 없을지 의문이 생길수 있습니다. 

Deepmind팀은 2017년 **Rainbow**라는 알고리즘을 발표하며 상기된 해결책들과 함께 3가지의 해결책들을 추가하여 학습을 진행하였습니다.

현재도 활발히 연구되고 있는 3가지 해결책들을 간략히 설명하면
* **Prioritized Experience Replay:**  Replay buffer에서 loss가 큰 sample들 위주로 sampling하여 학습의 속력을 가속화.
* **Distributional RL:**  Q network가 단순히 Q-value를 학습하는 것이 아닌, distribution을 학습하여 stochasticity가 높은 환경에 잘 대처.
* **Noisy Nets:** 효과적인 exploration을 위해 $$epsilon$$-greedy가 아닌 마지막 layer의 weight에 학습가능한 noise를 가함.

<center><img src = "/assets/images/cs285/lecture8/rainbow.png" width = "400"></center>
<center>Median human-normalized performance across 57 Atari games.</center><div style = "margin-bottom:12px"></div>

보시는 것과 같이 이러한 다양한 해결책들이 함께 사용될 때 성능을 크게 향상시켜 주었고, Rainbow DQN은 DQN과 더불어 강화학습에서 baseline으로서 보편적으로 사용되는 알고리즘중 하나입니다. 


## Q-learning with continuous action spaces

지금까지 봤던 DQN 알고리즘은 사실 discrete action space에서 사용할 수 있는 알고리즘이였습니다.
Action space가 continuous하다면 어떠한 문제점들이 생길까요?

$$ y = r + \gamma \, max_{a^{\prime}}Q_{\phi}(s^{\prime},a^{\prime}) $$

위와 같이 DQN의 target을 생성하는데 있어 $$Q_{\phi}(s,a)$$을 모든 action에 대하여 계산하는 것은 사실상 불가능합니다.

### Discretization

가장 쉽게 생각할 수 있는 해결책은 continuous한 action space를 discretize하여 discrete action space로 만드는 것입니다. 
이 방법은 정말로 simple하지만 값이 정확하지 않고, 값의 정확성을 위해 discretization을 늘리게 되면 계산의 복잡성이 크게 증가하여 사용하는 것이 쉽지 않습니다.

### NAF: Normalized Advantage Function

Levine 교수님은 2016년 ICML에서 $$Q$$ function을 최적화하기 쉬운 함수로 근사해서 학습하는 방법을 제안하셨습니다.
$$Q$$를 위가 볼록한 2차 함수로 근사한 뒤, 2차 함수의 극댓점을 action으로 선택하는 것입니다.

어떻게 $$Q$$를 2차 함수로 근사하는지 차근차근 식을 들여다보도록 하죠.

우선 dueling architecture와 같이 $$Q$$를 $$V$$와 $$A$$로 표현할 수 있습니다.

$$Q(s,a) = V(s) + A(s,a) $$

우리는 $$ A(s,a) $$가 optimal action에서 0, 그 외의 action에 대해서는 음의 값을 가진다는 것을 알고 있습니다. Optimal action $$\mu(s)$$에서 0의 극댓값을 가지는 2차 함수로 $$A(s,a)$$ 테일러 근사해보겠습니다.

$$ A(s,a) = -\frac{1}{2}(a - \mu(s))^T H(s) (a - \mu(s))$$

여기서 $$H(s)$$는 $$A(s,a)$$의 2차 미분행렬인 Hessian matrix입니다. $$H(s)$$는 positive-definite squre matrix이기 때문에 cholesky분해를 통해 low-traingular matrix $$L(s)$$로 분해할 수 있습니다.

$$H(s) = L(s)^T L(s)$$

이제 저희의 network가 $$\mu(s), L(s), V(s)$$를 알 수 있다면 $$Q(s,a)$$를 다시 계산할 수 있게 됩니다.

<center><img src = "/assets/images/cs285/lecture8/naf.png" width = "450"></center>

실제 hessian matrix $$H(s)$$를 사용하지는 않기 때문에, $$P(s) = L(s)^T L(s)$$로 notation을 바꾸어 표기하였습니다.

이렇게 만든 neural network에서 simulation시의 policy는 아래와 같습니다.

$$ \pi(a|s) = \mu(s) + noise $$

이후 simulation으로부터 학습을 진행할 때는 아래의 오차를 최소화하는 형태로 진행되게 됩니다.

$$ Loss = \vert \vert Q(s,a) - Q(s,\mu(s)) \vert \vert^2 $$

### DDPG: Deep Deterministic Policy Gradient

위의 NAF방식의 장점은 2차 함수로 $$Q$$를 근사하기 때문에 학습이 효율적일수 있으나, 함수의 표현력이 떨어지기 때문에 복잡한 task에서는 좋지못할 성능을 낼 수 있습니다. 
함수의 표현력을 높이는 가장 단순한 방법은 어떤것이 있을까요? 주어진 $$\mu(s)$$에 대하여 $$Q$$를 계산하는 neural network를 만드는 것입니다.

$$ \mu(s) = max_a Q(s,a)$$인 $$\mu$$를 학습하는 actor와 $$a$$와 $$s$$가 주어졌을때 $$Q(s,a)$$를 예측하는 critic으로 분리하는 것입니다.

<center><img src = "/assets/images/cs285/lecture8/ddpg.png" width = "600"></center><br>

그럼 critic의 loss로부터 actor의 파라미터를 학습을 진행하면, actor가 정말 optimal action $$\mu(s)$$를 찾아낼 수 있을까요?

David silver 교수님은 2014년 논문 Deterministic Policy Gradient Algorithms을 통해 아래의 deterministic policy gradient가 기존의 policy gradient와 equivalent함이 증명하였습니다.

$$ J(\mu_{\theta}) = E[R(s, \mu_{\theta})]$$

$$ \nabla_{\theta} J(\mu_{\theta}) = E[ \nabla_{\theta} \mu_{\theta}(s) \nabla_{a} Q(s,a)|_{a= \mu_{\theta}} ]$$

따라서, DDPG 알고리즘은 기존의 actor-critic 알고리즘에 DQN의 다양한 trick들을 활용가능한 알고리즘이 되는 것입니다.

## Summing up

1. Problems & Solutions of DQN
* Correlated samples $$\rightarrow$$ Replay Buffer
* Moving target $$\rightarrow$$ Target Network
* Maximization bias $$\rightarrow$$ Double Q-learning
* Ambiguous attention $$\rightarrow$$ Dueling architecture
* High bias, Low variance $$\rightarrow$$ N-step returns

2. DQN with continuous action space
* Discretization
* NAF, quadratic Q-value approximation
* DDPG, deterministic actor-critic

<br>

이번 강의에서는 DQN 알고리즘의 다양한 문제점들과 그에 따른 해결책들을 살펴보았습니다. DQN 알고리즘의 이런 다양한 개선에도 불구하고 여전히 학습 과정의 stability는 큰 문제로 남아있습니다. 
다음 강의에선 Deep learning과 결합한 policy gradient가 어떻게 발전해 나가고 있는지 살펴보도록 하겠습니다. 감사합니다.

## Related Papers
* (DQN) Playing Atari with Deep Reinforcement Learning. Mnih et al. NIPS 2015.
* (DQN) Human-level control through deep reinforcement learning. Mnih et al. Nature 2015.
* (Double DQN) Deep Reinforcement Learning with Double Q-learning. Hasselt et al. AAAI 2016.
* (Dueling DQN) Dueling Network Arhictectures for Deep Reinforcement Learning. Wang et al. ICML 2016.
* (Rainbow) Rainbow: Combining Improvements in Deep Reinforcement Learning. Hessel et al. AAAI 2018.
* (DPG) Deterministic Policy Gradient Algorithms. Silver et al. ICML 2014.
* (DDPG) Continuous control with Deep Reinforcement Learning. Lillicrap et al. ICLR 2016.
* (NAF) Continuous Deep Q-Learning with Model-based Acceleration. Gu et al. ICML 2016.



[dqn-2015]: https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning