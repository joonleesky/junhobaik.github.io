---
title:  "(작성중) [CS 285] Lecture 8: Deep RL with Q-Functions"
date:   2020-02-25 12:04:36 +0900
tags:
  - CS285
---
이번 강의에선 deep learning과 다양한 trick들을 이용해서 어떻게 현실의 문제들에 Q-learning을 적용할 수 있는지 살펴보도록 하겠습니다. CS 285 강의를 기본으로 하되 흐름에 맞게 내용들을 추가하고 재배치했음을 밝힙니다. 부족한점에 대한 지적과 질문은 자유롭게 댓글로 남겨주세요.

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

## Deep Q-network

지난 시간에는 Q-value를 학습하는 것을 통해 좋은 policy를 찾아나가는 Q-learning 알고리즘을 살펴보았습니다.

<center><img src = "/assets/images/cs285/lecture7/qlearn.png" width = "450"></center><br>

여기서 function approximator $$\phi$$로 neural network를 사용하는 알고리즘을 **DQN(Deep Q-network)**라고 부르곤 합니다. 하지만 위의 알고리즘은 학습 과정이 너무나 unstable해서 $$Q_{\phi}(s,a)$$가 잘 수렴하지 않았고, 수렴하더라도 local-optima에 쉽게 빠지곤 하였습니다. 
위의 알고리즘이 왜 그러한 문제점들을 겪는지, 그러한 문제점들을 해결할 수 있는 어떤 해결책들이 있는지 하나씩 살펴보도록 하겠습니다.

### Replay Buffer

첫번째로 살펴볼 문제는 **Correlated samples**입니다. 

### Target Network

### Double Q-learning

### Multi-step returns

## Q-learning with continuous action spaces

### NAF: Normalized Advantage Function

### DDPG: Deep Deterministic Policy Gradient

## Summing up

<br>



[sutton]: http://www.incompleteideas.net/book/the-book-2nd.html
[andrewng_mvn]: https://www.youtube.com/watch?v=JjB58InuTqM
[karpathy_pong]: http://karpathy.github.io/2016/05/31/rl/
[ce_mle]: https://medium.com/konvergen/cross-entropy-and-maximum-likelihood-estimation-58942b52517a