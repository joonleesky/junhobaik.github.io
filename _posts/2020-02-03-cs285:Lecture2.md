---
title:  "[CS 285] Lecture 2: Supervised Learning of Behaviors"
date:   2020-02-05 12:04:36 +0900
tags:
  - CS285
---

이번 포스팅에서는 강화학습 알고리즘이 해결하고자 하는 **sequential decision making problem**을 살펴보고 이를 **supervised learning**을 통해서는 어떻게 해결할 수 있는지 살펴봅니다. CS 285 강의를 기본으로 하되 흐름에 맞게 내용들을 추가하고 재배치했음을 밝힙니다. 부족한점에 대한 지적과 질문은 자유롭게 댓글로 남겨주세요.

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

## Sequential decision problem

이전 강의에선 강화학습 알고리즘이 sequential decision을 모델링하고 평가하기 위해 사용되는 알고리즘이라고 말씀 드렸습니다. 그렇다면 과연 sequential decision problem이 정확히 무엇인지 formal한 terminology와 notation을 이용해서 정의해보도록 하겠습니다.

<center><img src = "/assets/images/cs285/lecture2/sqd.png" width = "650"></center><br>

Sequential decision problem이란 저희의 agent가 매 시간 $$ t $$ 마다 observation $$ o_t $$을 input으로 받아서 action $$a_t$$을 선택해야하는 문제를 말합니다.  여기서 주어진 observation에 대하여 어떠한 action을 선택할지 결정하는 함수를 저희는 policy라고 말하는데, 주어진 상황이 fully-observable할 때 observation대신에 state라는 notation을 사용합니다. Optimal한 policy를 찾는데 있어서 우리의 input이 observation인지 state인지를 구분하는 것은 몹시 중요합니다. 

<center><img src = "/assets/images/cs285/lecture2/pgm.png" width = "650"></center><br>

위의 sequential decision problem을 probabilistic graphical model로 도식화하면 위와 같은 bayes network로 표현될 수 있습니다. 저희의 input이 observation인지 state를 구분하는것이 중요한 이유는, 아래의 증명과 같이 state들은 conditional independence를 만족하는 한편 observation들은 conditional independence를 만족하지 않기 때문입니다.

<div style="font-size: 90%">
$$
\begin{align*}
  P(s_1 \vert s_2, s_3) &= \: \cfrac{P(s_1, s_2, s_3)}{P(s_2, s_3)} \\[8pt]
 &= \cfrac{P(s_3, \vert s_2)P(s_2, \vert s_1)P(s_1)}{\sum_{s1} P(s_1,s_2, s_3)} \\[8pt]
 &= \cfrac{P(s_3, \vert s_2)P(s_2, \vert s_1)P(s_1)}{P(s_3 \vert s_2) \sum_{s1} P(s_2 \vert s_1) P(s_1)} \\[8pt]
 &= \cfrac{P(s_2, \vert s_1)P(s_1)}{P(s_2)} \\[6pt]
 &= P(s_1 \vert s_2)
\end{align*}
$$
<br></div>

**따라서, conditional independence를 만족하는 state는 markov property를 만족하지만 observation은 markov property를 만족하지 못하게 됩니다.** 이는 fully observable하지 않은 환경에서 좋은 policy를 찾기 위해선 history정보 역시 고려해주어야 한다는 것을 의미합니다.

Bayes network와 conditional independence를 좀 더 자세히 알고 싶으신 분은 [CS 188: Introduction to Artificial Intelligence, Lecture 7: Bayesian Network][bayes_net] 의 노트를 참고해주세요.

## Imitation learning

이제 본격적으로 sequential decision problem문제 중 가장 대표적인 자율주행을 imitation learning을 통해서 푸는 방법을 알아보도록 합시다. 여기서 imitation learning, supervised learning, behavioral cloning은 모두 동일한 학습 방법으로 받아들이셔도 됩니다. Supervised learning을 진행하기 위해서는 우선 training data가 필요하겠죠? 아래와 같이 전방에 카메라를 부착한 자동차를 운전자가 주행하게 한 뒤, 카메라의 화면을 observation으로 핸들의 방향을 action으로 지정하여 training data를 생성합니다. 이후, supervised learning방식을 통해서 training data로 policy를 학습시킵니다.

<center><img src = "/assets/images/cs285/lecture2/imitation_car.png" width = "650"></center><br>

하지만 supervised learning의 방식으로 학습된 모델은 **distributional drift**라는 문제점을 가져오게 됩니다. 아래 그림과 같이 학습된 model이 실수를 저지르게 되면 model은 training data에는 보지 못했던 observation들에 노출되게 됩니다. 자연스럽게 이러한 observation에서는 좋지 못한 action을 선택하게 되고, 또 다시 기존의 training data와는 크게 다른 observation에 노출되며 오류가 반복적으로 누적되는 것입니다.

<center><img src = "/assets/images/cs285/lecture2/trajectory.png" width = "550"></center><br>

그럼에도 불구하고 Levine교수님은 다양한 heuristic한 trick들을 통해서 충분히 현실세계에 적용가능한 모델을 학습시킬 수 있다고 말하십니다. NVIDIA는 자율주행 자동차를 학습시키는데 있어, 자동차의 좌측 전방과 우측 전방에 카메라를 한대씩 더 설치하고 좌측 카메라의 label이 되는 action에는 우측으로 bias를, 우측 카메라의 label에는 좌측으로 bias를 주어 training data를 생성하였습니다. 이렇게 학습된 자율주행 자동차가 예정된 trajectory에서 왼쪽으로 벗어나게되면 정면 카메라는 학습된 모델의 좌측 카메라의 시야를 보게되고, action의 우측 bias로 인하여 우회전을 하게 되어 distributional drift를 어느정도 극복할 수 있게 됩니다. 

<center><img src = "/assets/images/cs285/lecture2/car_trick.png" width = "250"></center><br>

### DAgger

하지만, 자율주행에서만 적용가능한 이런 heuristic한 방법이 아니라, 보다 general한 알고리즘적 해결법이 있을까요? Stephane Ross는 2011년 [A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning][DAgger] 에서 DAgger라는 아주 단순한 해결법을 제시합니다. 기존의 방법으로 학습된 policy로 테스트를 해보며 추가적인 data를 수집하고 domain expert를 통해서 해당 data에 대한 label을 재부여하는 것입니다.

<center><img src = "/assets/images/cs285/lecture2/dagger.png" width = "600"></center><br>

그렇다면, 과연 DAgger 알고리즘은 기존의 behavioral cloning방법보다 얼마나 잘 동작할까요?

### Theoretical analysis

DAgger알고리즘이 behavioral cloning에 비해 얼마나 distributional drift를 개선할 수 있는지, theoretical analysis을 통해서 알고리즘의 upper bound를 평가해보겠습니다. 

평가를 위해서 모델의 학습된 policy가 실제 training data와 mismatch가 일어날 확률의 upper bound를 정의하고, mismatch가 일어날때마다 1이라는 cost가 발생한다고 전제하겠습니다.

$$ \pi_{\theta}(a \ne \pi^{\star}(s) \vert s) \leq \epsilon $$

$$ cost(s,a) = \begin{cases} 0 & \text{if } a = \pi^{\star}(s) \\ 1 & \text{otherwise} \\ \end{cases}$$

***Behavioral cloning***

우선 위와 같은 전제 아래에서 현재 policy에서의 state distribution을 정의해보도록 하죠.

$$ p_{\theta}(s_t) = (1 - \epsilon)^t p_{train}(s_t) + (1 - (1 - \epsilon)^t) p_{mistake}(s_t) $$

각 timestep에서의 $$ p_{\theta} $$의 state distribution은 timestep $$ t $$까지 mistake가 발생하지 않았을 때의 distribution과 mistake가 발생했을 때의 distribution으로 나누어 나타낼 수 있습니다. 이제, 정의된 distribution에서의 cost의 expectation의 upper bound를 계산해보도록 하겠습니다.

<div style="font-size: 100%">
$$
\begin{align*}
 \sum_{t} E_{p_{\theta}(s_t)}[c_t] &= \sum_t \sum_{s_t} p_{\theta}(s_t)c_t(s_t) \\[8pt]
 & \leq \sum_t \underbrace{\sum_{s_t} p_{train}(s_t)c_t(s_t)}_{\text{policy mismatch is bounded by } \epsilon} + \underbrace{\sum_{s_t} (p_{\theta}(s_t) - p_{train}(s_t))c_t(s_t)}_ {\text{total variation divergence is bounded by } 2 \epsilon t}\\[8pt]
 & \leq \sum_t \epsilon + 2 \epsilon t \\[6pt]
 & \leq O(\epsilon t^2) \\[6pt]
\end{align*}
$$
</div>

<div style="font-size: 90%">
$$
\begin{align*} \\
 \sum_{s_t} (p_{\theta}(s_t) - p_{train}(s_t))c_t(s_t) &= \vert (1 - \epsilon)^t p_{train}(s_t) + (1 - (1 - \epsilon)^t) p_{mistake}(s_t) - p_{train}(s_t)\vert \\
 &= (1 - (1 - \epsilon)^t) \underbrace{\vert p_{mistake}(s_t) - p_{train}(s_t)\vert}_{\text{two probability distribution's difference is at most 2}}\\[6pt]
 & \leq (1 - (1 - \epsilon)^t) \, 2 \\[6pt]
 & \leq 2 \epsilon t \\[6pt]
\end{align*}
$$
</div>

그렇다면, DAgger의 upper bound는 어떻게 바뀔까요?

***DAgger***

현재 policy에서의 state distribution은 반복적으로 train data에 aggregate되기 때문에 두 distribution은 aggregation이 무한하다는 가정하에 같은 distribution이 됩니다.

$$ p_{\theta}(s_t) \approx p_{train}(s_t) $$

위와 같은 전제조건 속에서의 cost의 upper bound는 아래와 같습니다. 

<div style="font-size: 100%">
$$
\begin{align*}
 \sum_{t} E_{p_{\theta}(s_t)}[c_t] &= \sum_t \sum_{s_t} p_{\theta}(s_t)c_t(s_t) \\[6pt]
 &= \sum_t \sum_{s_t} p_{\theta}(s_t)c_t(s_t) \\[6pt]
 & \leq \sum_t \epsilon \\[6pt]
 & \leq O(\epsilon t) \\[6pt]
\end{align*}
$$
</div>

따라서, behavioral cloning은 DAgger와 달리 주어진 trajectory의 길이에 quadratic하게 cost가 증가하기 때문에 trajectory의 길이가 길수록 distributional drift문제는 급격하게 심화될 수 있습니다.

## Case studies of deep imitation learning

저희는 theoretical analysis를 통해 DAgger 알고리즘을 통한 training data의 확보가 imitation learning의 성능을 향상 시킬 수 있음을 확인해 보았습니다. 그렇지만, domain expert가 반복적으로 정확한 label을 부여해주어야 하는 것은 여전히 중대한 limitation으로 남아있습니다. 그렇다면 처음부터 모델을 정말 잘 학습시켜 training trajectory에서 크게 벗어나지 않게 할 수 는 없을까요? 모델을 잘 학습시키는데 있어서 장애물은 어떤것들이 있을까요?

### Non-markovian behavior

첫번째는 장애물은 바로 **non-markovian behavior**입니다. 앞서 sequential decision problem의 probabilistic graphic model에서 살펴본것과 같이 저희의 input이 state가 아니라 observation일 경우에는 markov property를 만족하지 않게 됩니다. 따라서, 모델의 정확도를 높이기 위해서는 현재의 observation만이 아니라, observation의 history까지 고려해주어야 합니다. 하지만 아래 그림과 같이 모든 history를 input으로 사용한다면, input의 크기가 너무나 커지기 때문에 현실적인 해결책이 아닐 것입니다. 

<center><img src = "/assets/images/cs285/lecture2/history.png" width = "550"></center><br>

따라서, 시계열 형태의 데이터에서 memory의 역할을 수행하는 **LSTM**과 같은 모델은 좋은 대안이 될 수 있습니다.

하지만, history정보를 input에 활용하는 것은 사건들의 인과관계를 올바르게 연결짓지 못하는 **causal confusion** 문제를 오히려 심화시킬 수도 있습니다. Pim de Haan이 [Causal Confusion in Imitation Learning][casual_confusion]에서 차용한 예제를 활용하여 casual confusion문제를 살펴보도록 하죠.

<center><img src = "/assets/images/cs285/lecture2/casual.png" width = "350"></center>
<center><i>Casual confusion from Haan et al</i></center><div style = "margin-bottom:20px"></div>

위 그림과 같이 브레이크를 밟는 순간마다 노란 불이 들어오는 이상한 자동차가 있습니다. 운전자는 보행자가 앞에 서있을때 자연스럽게 브레이크를 밟게되고, 차는 동시에 노란 불로 감싸지게 됩니다.  보행자가 앞에 서있는 상황만이 아니라, 차가 옆에서 끼어들거나 고라니가 갑자기 길을 막아서는 다양한 상황들에서도 운전자는 브레이크를 밟게되고 노란불 역시 들어오게 됩니다. 이렇게 형성된 training data로 학습시킨 모델은 (보행자가 보인다) → (브레이크를 밟는다)→(노란 불빛이 들어온다) 라는 함수보다는 (노란 불빛이 들어온다) → (브레이크를 밟는다)라는 함수를 학습하게 됩니다. **History 정보를 활용할 수록 노란 불빛과 브레이크의 상관관계는 더욱 뚜렷해지고, 잘못된 인과관계를 학습할 확률은 올라가게 됩니다.**  

실제로 Dequan Wang은 [Monocular plan view networks for autonomous driving][gta_v]에서 history를 활용한 imitation learning이 GTA-V 환경에서 validation perplexity를 향상시켰음에도 불구하고 주행 성능을 저해시켰다는 것을 아래 표를 통해 보여주었습니다.

<center><img src = "/assets/images/cs285/lecture2/gta.png" width = "400"></center>
<center><i>GTA-V imitation learning results from Wang et al</i></center><div style = "margin-bottom:20px"></div>


### Multi-modal behavior

두번째로 중요한 장애물은 training data의 **multi-modality**입니다.

<center><img src = "/assets/images/cs285/lecture2/multi_modal.png" width = "250"></center><br>
<center><i>multi-modality in training data</i></center><div style = "margin-bottom:20px"></div>

만약 저희의 supervised model이 gaussian function을 위와 같은 action에 mapping하는 형태로 학습이 된다면 나무를 만났을 때 어떻게 대처하게 될까요? Training data에서는 우측으로 절반, 좌측으로 절반을 갔음에도 불구하고 저희의 model은 오히려 중앙으로 돌진하는 불상사가 발생하게 됩니다. 이런 문제점을 해결할 수 있는 방법중 하나는 **gaussian mixture model**을 활용하여 왼쪽으로 가는 policy와 오른쪽으로 가는 policy를 분리하여 학습하는 것입니다. 

Policy function의 형태가 너무 복잡해서 continuous function을 학습하는 것이 용이하지 않을 경우는 어떻게 할까요? Action space를 discretize하는 것 역시 하나의 방안일 수도 있지만, action의 dimension이 크다면, action space는 dimension에 exponential하게 증가하기 때문에 현실적인 방안이 아닐 것입니다. 

<center><img src = "/assets/images/cs285/lecture2/auto_reg.png" width = "350"></center>
<center><i>auto-regressive discretized neural network</i></center><div style = "margin-bottom:20px"></div>

따라서, 각 action의 dimension을 independent하게 학습하는 것은 좋은 대안이 될 수 있습니다. Action의 dimension별로 discretize를 한 뒤, sampling한 action을 통해 다음 dimension의 action을 선택하는 **auto-regressive discretization**역시 하나의 대안입니다.

최근 다양한 task에서 각광받고 있는 **unsupervised method**를 활용하는 것도 model의 성능을 향상시킬 수 있습니다. Variational auto-encoder와 같은 latent variable model로 학습된 weight는 model의 훌륭한 initialization point로 활용할 수도 있습니다. Latent variable model에 대한 자세한 내용은 [Lecture 13: Variational Inference and Generative Models][lvm]을 참고해주세요.

## Summing up

이번 강의에서 다룬 내용을 정리하자면 아래와 같습니다.

1. Imitation learning is one solution for the sequential decision problem
2. Still... there exists several limitations and various researches are under way 
* Distributional drift → Hacks (left / right cameras), DAgger
* Non-markovian behavior → Use history (lstm models), but this can rather increase causal confusion
* Multi-modal behavior → Gaussian mixture models,
* Huge action dimensions → Auto-regressive discretization
* Hard to train → Utilize unsupervised methods

<br>
이번 강의에서 살펴본것과 같이, supervised learning method를 sequential decision problem에 다양하게 활용되고 있지만, 지도학습의 특성에서 기인한 많은 문제점들이 뒤따릅니다. 다음 강의에선 sequential decision problem을 조금 더 formal하게 정의하고 이를 reinforcement learning을 이용해 어떻게 해결할 수 있는지 알아보겠습니다.

[bayes_net]: https://inst.eecs.berkeley.edu/~cs188/sp19/assets/notes/n6.pdf
[DAgger]: https://www.cs.cmu.edu/~sross1/publications/Ross-AIStats11-NoRegret.pdf
[casual_confusion]:https://papers.nips.cc/paper/9343-causal-confusion-in-imitation-learning.pdf
[gta_v]: https://arxiv.org/abs/1905.06937
[lvm]:http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-13.pdf