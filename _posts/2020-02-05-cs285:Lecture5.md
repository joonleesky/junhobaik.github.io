---
title:  "[CS 285] Lecture 5: Policy Gradients"
date:   2020-02-09 12:04:36 +0900
tags:
  - CS285
---
이번 포스팅에서는 강화학습의 가장 대표적인 알고리즘인 **policy graident**를 살펴보고, 이에 수반되는 다양한 문제점들과 해결책들을 살펴봅니다. CS 285 강의를 기본으로 하되 흐름에 맞게 내용들을 추가하고 재배치했음을 밝힙니다. 부족한점에 대한 지적과 질문은 자유롭게 댓글로 남겨주세요.

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

## Policy gradient

알고리즘을 본격적으로 알아보기 이전에 reinforcement learning의 objective를 다시 한번 돌아보도록 하죠.

### Goal of the reinforcement learning

<img src = "/assets/images/cs285/lecture5/goal.png" width = "800">

강화학습이 해결하고자 하는 문제는 위와 같은 state, action, transition probability, reward function으로 이루어진 MDP (markov decision process)입니다. 저희의 목표는 위와 같은 MDP에서 cumulative rewards를 maximize할 수 있는 policy를 찾는 것이었습니다. 지금 주어진 policy가 $$ \pi_{\theta} (a \vert s) $$ 라고 한다면, 이 policy속에서의 trajectory의 likelihood는 아래와 같이 나타낼 수 있습니다.

$$ p_{\theta}(\tau) = p_{\theta} (s_1,a_1,....,s_T,a_T) = p(s_1) \prod^{T}_{t=1}\pi_{\theta}(a_t|s_t)p(s_{t+1}\vert s_t,a_t)  $$

처음에 주어진 initial state의 probability distribution에서 episode가 끝날 때 까지, policy의 likelihood와 transition probability를 계속 곱해가는 것이지요. 저희의 objective 이러한 trajectory likelihood distribution속에서 cumulative rewards를 maximize할 수 있는 policy를 찾는 것으로 바꿔서 생각할 수 있으며,

$$ {\theta}^{\star} = arg \,max_{\theta} \, E_{\tau \sim p_{\theta} (\tau)} [\sum_t r(s_t,a_t) ]  $$

각 timestep에서의 (state, action) tuple의 distribution에서 reward를 maximize하는 것으로 바꿔 사용할 수도 있습니다. 알고리즘의 목적에 따라 두 가지의 수식이 번갈아서 사용되기 때문에 반드시 두가지 식 모두 익숙해질 필요가 있습니다.

$$ \theta^{\star} = arg \,max_{\theta} \, \sum_t^{T} E_{(s_t,a_t) \sim p_{\theta} (s_t,a_t)}[ r(s_t,a_t) ]  $$

### Policy gradient

이제 본격적으로 policy gradient 알고리즘에 들어가보도록 하겠습니다. 알고리즘은 정말 간단합니다. 주어진 objective function $$ J(\theta) =  E_{\tau \sim p_{\theta} (\tau)} [\sum_t r(s_t,a_t) ] $$를 $$ \theta $$ 에 대해 미분한 뒤, 계산된 gradient를 이용해서 gradient ascent를 취해주면 됩니다. 

그렇다면 trajectory distribution의 expectation은 어떻게 구할 수 있을까요? 단순합니다! 주어진 policy로 simulation을 반복해서 $$ J(\theta) $$를 근사하는 것이지요.

$$ J(\theta) =  E_{\tau \sim p_{\theta} (\tau)}[\sum_t r(s_t,a_t)] \approx \sum_i \sum_t r(s_{i,t}, a_{i,t}) $$

High-level에서 살펴본 policy gradient 알고리즘은 아래와 같습니다.
1. Simulation을 통해 $$ J(\theta) $$ 를 근사한다. 
2. $$ \nabla_{\theta} J(\theta) $$ 를 계산한다.
3. Gradient ascent를 수행한다.
<br>

### Policy differentiation

이제 단 1가지의 절차만이 남겨져있습니다. $$ \nabla \theta J(\theta) $$ 를 구하면 되는것이지요. 저도 강화학습을 처음 공부할 때 이 부분에서 큰 장벽을 느꼇었고, 쉽지 않은 유도 과정이라고 생각합니다. 하지만, 천천히 시간을 두시고 살펴보신다면 모두 충분히 이해하실 수 있다고 믿습니다.

$$ \sum_t r(s_{i,t}, a_{i,t}) $$은 앞으로의 notation의 편의를 위해 $$ r(\tau) $$ 로 표기하겠습니다.

$$ \nabla_{\theta} J(\theta) = \nabla_{\theta} E_{\tau \sim p_{\theta} (\tau)} [ r(\tau) ] =  \int \nabla_{\theta} \pi_{\theta} (\tau) r(\tau) d\tau $$

Probability distribution에서의 expectation은 expectation의 정의에 따라 $$ r(\tau) $$ weighted된 probability distribution의 integral로 표현할 수 있고, $$\nabla_{\theta} $$는 $$\int $$의 안으로 들어갈 수 있습니다.

$$ \int \nabla_{\theta} \pi_{\theta} (\tau) r(\tau) d\tau = \int \pi_{\theta} (\tau) \nabla_{\theta} \log \pi_{\theta} (\tau) \, r(\tau) d\tau = E_{\tau \sim p_{\theta} (\tau)} [ \nabla_{\theta} \log \pi_{\theta} (\tau) r(\tau) ]$$

$$ \nabla_{\theta} \log \pi_{\theta} (\tau) = \cfrac{\nabla_{\theta} \pi_{\theta} (\tau)}{\pi_{\theta} (\tau)} $$ 를 만족하기 때문에 $$ \pi_{\theta} (\tau) $$ 를 바깥으로 다시 꺼내 expectation의 형태로 되돌립니다. 

이제 앞서 정의했던 trajectory의 likelihood에 log를 취해 볼까요?

$$ \log p_{\theta}(\tau) = \log p_{\theta} (s_1,a_1,....,s_T,a_T) = \log p(s_1) + \sum^{T}_{t=1} \log \pi_{\theta}(a_t|s_t) + \log p(s_{t+1}\vert s_t,a_t)  $$

<center><img src = "/assets/images/cs285/lecture5/pg_derv1.png" width = "380"></center><br>

자연스럽게 $$ \theta $$ 와 무관한 항들은 소거되고, policy gradient는 최종적으로 아래와 같이 유도되게 됩니다.

$$ \nabla_{\theta} J(\theta) =  E_{\tau \sim p_{\theta} (\tau)} [ (\sum^{T}_{t=1} \nabla_{\theta} \log \pi_{\theta} (a_t \vert s_t)) (\sum^{T}_{t=1} r(s_t,a_t)]$$

결국 policy gradiet를 활용한 **REINFORCE** algorithm 은 아래와 같습니다.

<center><img src = "/assets/images/cs285/lecture5/reinforce.png" width = "450"></center><br>

### Analysis

수식에 대한 유도는 끝났지만, policy gradient의 수식의 의미를 직관적으로 파악하는 것은 쉽지 않습니다. 
<center><img src = "/assets/images/cs285/lecture5/rl_vs_sl.png" width = "700"></center><br>

Levine 교수님은 maximum likelihood의 gradient와 policy gradient의 차이를 통해 훌륭한 직관을 제시해 주십니다. <br> 2장에서 다룬 것과 같이 supervised learning 방식으로 model을 학습시킨다면 cross-entropy loss를 minimize하는 것이 가장 보편적인 선택지 일 것입니다. Model이 cross-entropy loss를 minimize하는 것은 data의 likelihood를 maximize하는것과 동일하기 때문에 위와 같이 표현될 수 있습니다. 위의 두 식이 상당히 닮아보이지 않나요? **Policy gradient를 통한 gradient ascent는 기존의 maximum likelihood 방식과 동일하게 simulation된 data를 matching하지만 cumulative rewards만큼 gradient에 가중치를 부여하는 것입니다.** <br>

Cross-entropy loss와 maximum likelihood의 관계에 대해 궁금하신 분은 [Cross-entropy and Maximum Likelihood Estimation][ce_mle]를 참조해주세요.

<center><img src = "/assets/images/cs285/lecture5/karpathy.png" width = "700"></center><br>

우리 모두가 익숙한 cs231n의 강의자셨던 andrej karpathy도 이 수식을 이해하기 위한 훌륭한 그림을 그려주셨습니다. Trajectory의 길이가 1인 단순한 게임을 한번 생각해보도록 하죠. 왼쪽의 그림의 distribution이 저희가 구하고 싶은 $$ \pi_{\theta} (a_t \vert s_t) $$이고 파랑색 점들은 이런 distribution에서 samplinge된 action이라고 하지요. 화살표는 각 sample에서의 $$ \nabla_{\theta} \log \pi_{\theta} (a_t \vert s_t)) $$과 같습니다. 중간 그림과 같이 왼쪽 아래 방향의 action은 $$ +1 $$의 reward가 나머지의 action들에 대해서는 $$ -1 $$의 reward가 발생되었습니다. 저희의 policy gradient식을 적용해서 gradient ascent를 진행한다면 우측 그림과 같이 $$ +1 $$의 reward가 발생하는 지역으로 distribution이 이동하게 됩니다.

이제 policy gradient가 무엇이고 어떠한 intuition이 바탕에 있는지 조금 감이 오셨나요? Karpathy의 blog에도 supervised learning과 policy gradient에 대한 비교를 해논 정말 좋은 글이 있으니 모호함이 느껴지신다면 꼭 읽어 볼것을 추천드립니다! [Deep Reinforcement Learning: Pong from Pixels][karpathy_pong]

## Implementation

이제 policy gradient의 수식도 이해했고, REINFORCE 알고리즘도 이해했으니 코드를 얼른 구현해봐야겠죠! 사실 저 또한 작년에 수식을 이해한 뒤 알고리즘을 바로 구현해보려 하였으나, model을 어떻게 구현해야하고 gradient ascent를 어떻게 반영해야하는지 벽에 가로막혀 많은 시행착오를 겪었습니다. REINFORCE 알고리즘을 어떻게 구현할지 차근차근 살펴보도록 하죠.

<center><img src = "/assets/images/cs285/lecture5/loop.png" width = "500"></center><br>

알고리즘의 전체적인 pipeline을 쉽게 이해하기 위해서 Levine교수님께서 제시해주신 reinforcement learning algorithm pipeline을 활용하도록 하겠습니다.

### Model

우선 본격적으로 학습을 시작하기전에 policy를 represent할 수 있는 model을 생성해야 합니다. Environment의 action space가 discrete하다면 가장 general한 선택은 action에 대한 logit을 뽑은 후에 softmax를 통해 $$ \pi_{\theta} (a_t \vert s_t) $$구하는 것일 겁니다.

<center><img src = "/assets/images/cs285/lecture5/discrete.png" width = "500"></center><br>

반면, environment의 action space가 continuous하고 action dimension이 1이라면 gaussian policy가 하나의 대안이 될 것입니다. Gaussian probability distribution의 파라미터는 $$ \mu $$ 와 $$ \sigma $$ 이기 때문에 model을 통해 2개의 파라미터를 학습시킵니다. 

<center><img src = "/assets/images/cs285/lecture5/continuous.png" width = "550"></center><br>


### Simulation

이제 policy도 생성했으니 simulation을 진행해 해볼까요? 아래 그림과 같이 주어진 episode수 만큼 simulation을 반복합니다.

<center><img src = "/assets/images/cs285/lecture5/simulate.png" width = "700"></center><br>

### Estimate policy gradient

다음으론 모아둔 data를 통해서 policy gradient를 추정합니다. 

<center><img src = "/assets/images/cs285/lecture5/estimate1.png" width = "700"></center><br>

저희가 추정해야 하는 $$ \nabla_{\theta} \log \pi_{\theta} (a_t \vert s_t)) $$는 model을 구현할 때 graph에 미리 생성해둡니다. Tensorflow, PyTorch로 구현한다면 정의해둔 loss에 따라 자동적으로 gradient가 구해지기 때문에 걱정하실건 없습니다. <br><br>

<center><img src = "/assets/images/cs285/lecture5/estimate2.png" width = "700"></center><br>

여기에 해당 episode에서 발생했던 reward를 모두 더해 weighting한다면 policy gradient를 구할 수 있게 됩니다.<br><br>

### Perform gradient ascent

그렇다면, loss는 어떻게 정의해두어야 할까요? 저희의 목표는 objective function $$ J(\theta) $$를 maximize하는 것이 목표기 때문에 loss를 $$ -J(\theta) $$로 정의하고 이것을 minimize하면 동일한 objective가 됩니다.

{% highlight python %}
def compute_loss(obs, act, weights):
    logp = get_policy(obs).log_prob(act)
    return -(logp * weights).mean()
{% endhighlight %}
<br>

### Tricks

Implementation에 대한 내용을 마치기전에 알아두시면 좋은 몇가지 trick이 있는데요, gassuain policy에서 학습한 $$ \mu $$ 와 $$ \sigma $$를 가지고 direct하게 action을 sampling하는 것은 불가능하다는 것입니다. 따라서 실제로 simulation을 진행할 때에는 아래와 같이 **re-parameterization trick**을 통해 action을 sampling하게 됩니다.

<center><img src = "/assets/images/cs285/lecture5/continuous_reparam.png" width = "550"></center><br>

OpenAI의 spinning up rl에서는 action dimension이 1보다 큰 continuous action space에서 **multivariate gaussian policy**를 학습시킬 때 아래와 같이 몇가지 주의사항을 당부해주고 있습니다. 

<center><img src = "/assets/images/cs285/lecture5/diagonal.png" width = "600" style = "margin-bottom:12px"></center>

1. 모든 action dimension이 independent하다고 가정하고 diagonal gaussian policy를 학습시켜라
2. $$ \sigma $$는 state에 conditional한 function으로 사용하지 않는 것이 좋을 수 있다
3. $$ \sigma $$를 학습하지 말고 constraint가 적은 $$ \log \sigma $$를 학습해라
<br>




이렇게 정말로 길었던 policy gradient에 대한 개괄적인 설명과 rough하게 implementation에 대한 내용을 마쳤습니다. 하지만 애석하게도, 이렇게 구현한 vanilla policy gradient는 몇가지 문제들로 잘 동작하지 않습니다. **이 알고리즘의 몇가지 문제점들과 해결책들을 같이 알아보도록 하죠.** <br><br>

## Problem 1: high variance

첫번째로 살펴볼 중요한 문제는 바로 high variance입니다. 아래에 있는 저 곡선이 저희의 initial policy의 trjaectory distribution이라고 하죠.

<center><img src = "/assets/images/cs285/lecture5/initial.png" width = "550"></center><br>

여기서 3가지의 각 trajectory에서 초록색 선과 같이 reward를 받는다면 오른쪽 점선과 같이 policy의 distribution이 변화하게 됩니다.

<center><img src = "/assets/images/cs285/lecture5/policy1.png" width = "550"></center><br>

이번에는 각 trajectory의 reward에 constant를 한번 더해보도록 하겠습니다. Constant를 더하더라도 결국 학습되는 policy는 이전과 같지만, 처음 update된 policy의 distribution은 아래와 같이 나오게 됩니다.

<center><img src = "/assets/images/cs285/lecture5/policy2.png" width = "550"></center><br>

따라서, policy가 이렇게 noisy하게 update되지 않게 하기 위해선 variance를 줄이는 것이 무엇보다도 중요합니다.

### Casuality

Variance를 줄이는 첫번째 해결책은 casuality를 도입하는 것입니다. $$ t $$ 번째의 timestep에서의 policy는 $$ t $$ 이전에 발생한 reward에 대한 어떠한 인과관계도 없습니다. 따라서, 각 timestep이후에 발생한 reward에 대해서만 합산해주는 것입니다.

$$ \nabla_{\theta} J(\theta) =  E_{\tau \sim p_{\theta} (\tau)} [ (\sum^{T}_{t=1} \nabla_{\theta} \log \pi_{\theta} (a_t \vert s_t)) (\sum^{T}_{t^\prime=t} r(s_t^\prime,a_t^\prime))]$$

### Baselines

이번에는 cumulative rewards에 constant를 빼주어 아래와 같이 objective function을 변화시키는 것입니다.

$$ \nabla_{\theta} J(\theta) =  E_{\tau \sim p_{\theta} (\tau)} [ (\sum^{T}_{t=1} \nabla_{\theta} \log \pi_{\theta} (a_t \vert s_t)) (\sum^{T}_{t=1} r(s_t,a_t) - b)]$$

그런데... 이렇게 constant값을 빼주어도 과연 괜찮은 것일까요? 

<center><img src = "/assets/images/cs285/lecture5/baseline.png" width = "700"></center><br>

정답은 yes입니다! 위의 식에서 보듯이 baseline을 도입하더라도 expectation은 그대로 동일하기 때문에 constant를 차감해주어도 결국은 같은 policy를 학습하게 되는것입니다. 

그렇다면 어떤 값을 baseline으로 사용하는 것이 좋을까요? Levine 교수님은 trajectory들의 average reward를 simple한 baseline으로 추천해주십니다. 사실 이 값을 cumulative rewards에서 빼주는 것은 몹시 직관적입니다. 다른 trajectory에 비해 평균적으로 더 좋은 trajectory의 distribution을 높이는 방향으로 학습시키는 것이기 때문입니다.

그렇다면 average reward를 baseline으로 사용하는 것이 theoretic하게 가장 variance를 줄일 수 있는지 확인해보도록 할까요?

<center><img src = "/assets/images/cs285/lecture5/var1.png" width = "550"></center><br>

Policy gradient의 variance는 위와 같이 유도될 수 있습니다. 이 값을 b에 대해 미분하여 최소점을 찾아보면 아래와 같습니다.

<center><img src = "/assets/images/cs285/lecture5/var2.png" width = "600" style = "margin-bottom:12px"></center>

<center><img src = "/assets/images/cs285/lecture5/var3.png" width = "130"></center><br>

제가 생각하는 위 baseline의 직관적인 의미는 해당 trajectory의 reward가 크더라도 trajectory로 가는 확률이 크지 않으면 update를 적게한다(?)로 이해하고 있습니다. 이런 baseline을 사용한다면 variance는 줄일 수 있어도 policy가 쉽게 overfitting하지 않을까 생각됩니다.<br>

최종적으로, casuality와 baseline을 도입한 policy gradient의 식은 아래와 같이 바뀌게 됩니다. 

$$ \nabla_{\theta} J(\theta) =  E_{\tau \sim p_{\theta} (\tau)} [ (\sum^{T}_{t=1} \nabla_{\theta} \log \pi_{\theta} (a_t \vert s_t)) (\sum^{T}_{t^\prime=t} r(s_t^\prime,a_t^\prime) - b)]$$

그럼에도 불구하고 실제 알고리즘을 학습시킬때는 여전히 variance가 매우 높기 때문에, 일반적인 지도학습에 사용하는것보다 **커다란 batch size(e.g. 1000, 10000)**를 사용하고 **cumulative reward를 batch에 대해서 normalize**를 하여 update하곤 합니다.

## Problem 2: on-policy

자 이제 끝이 보입니다. 혹시 지난시간에 policy gradient류의 알고리즘이 sample-inefficient하다고 말한것 기억나시나요? 이 section에서는 왜 policy gradient 알고리즘이 sample inefficient한지, inefficiency를 어떻게 극복할 수 있는지 다뤄보도록 하겠습니다.

$$ J(\theta) =  E_{\tau \sim \pi_{\theta} (\tau)} [ r(\tau) ] $$ 

처음에 정의한 reinforcement learning의 objective function으로 다시 돌아왔습니다. 여기서 문제점은 expectation이 현재의 policy $$ \pi_{\theta} $$에 expectation이 정의되어 있다는 점입니다. 그렇다면 과거의 policy $$ \bar\pi $$ 에서 sampling한 simulation은 전혀 사용하지 못하게 되는 것일까요?

### Importance sampling

Importance sampling을 활용해서 기존의 objective를 조금 다르게 표현해보도록 합시다.

$$  J(\theta) = E_{\tau \sim \pi_{\theta} (\tau)} [ r(\tau) ] = \int \pi_{\theta} (\tau) r(\tau) d\tau $$

$$ \int \pi_{\theta} (\tau) r(\tau) d\tau =  \int \bar \pi (\tau) \cfrac{\pi_{\theta}(\tau)}{\bar \pi (\tau)} r(\tau) d\tau = E_{\tau \sim \bar\pi(\tau)} [\cfrac{\pi_{\theta}(\tau)}{\bar \pi (\tau)} r(\tau) ] $$

이렇게 objective function이 바뀐다면, expectation이 $$ \bar \pi (\tau) $$에 걸려있기 때문에, 과거의 data를 사용할 수 있게 되는 것입니다. 그렇다면, 새롭게 등장한 $$ \cfrac{\pi_{\theta}(\tau)}{\bar \pi (\tau)} $$는 과연 어떤 의미를 가질까요?  각 trajectory에 대하여 과거의 policy $$ \bar\pi $$가 왼쪽, 현재의 policy $$ \pi_{\theta} $$가 오른쪽의 distribution을 따른다고 해보지요. 

<center><img src = "/assets/images/cs285/lecture5/two_dist.png" width = "500"></center><br>

분명히 오른쪽에 해당하는 trajectory들이 update를 많이 받아야 함에도 불구하고, $$ \bar \pi (\tau) $$에서 sampling된 data는 왼쪽에 쏠려있기 때문에 오른쪽의 trajectory는 적은 수의 update의 기회만을 받게 됩니다. 따라서, 오른쪽의 trajectory가 sampling되었을 때는 $$ \bar\pi $$로 나누어 때문에 큰 보폭으로 update를, 반면에 왼쪽의 trajectory가 sampling되었을 때는 $$ \pi_{\theta} $$ 곱하여 때문에 적은 보폭으로 update를 하는 것이지요.

이제 위의 수식으로부터 policy gradient를 estimate해보도록 하겠습니다. 우선 $$ \cfrac{\pi_{\theta}(\tau)}{\bar \pi (\tau)} $$ 부터 정리해보도록 하죠.

<center><img src = "/assets/images/cs285/lecture5/importance.png" width = "450"></center><br>

변형된 수식을 집어넣어 policy gradient를 구하면 아래와 같습니다.

$$ \nabla_{\theta} J(\theta) =  E_{\tau \sim \bar\pi(\tau)} [\cfrac{\pi_{\theta}(\tau)}{\bar \pi (\tau)} \nabla_{\theta} \log \pi_{\theta} (\tau) r(\tau)]$$

$$ =  E_{\tau \sim \bar\pi(\tau)} [\prod^T_{t=1} \cfrac{\pi_{\theta}(a_t \vert s_t)}{\bar \pi (a_t \vert s_t)} (\sum^{T}_{t=1}\log \pi_{\theta} (a_t \vert s_t)) (\sum^{T}_{t=1} r(s_t,a_t)]$$

그런데 여기서 문제가 있습니다. $$ \prod^T_{t=1} \cfrac{\pi_{\theta}(a_t \vert s_t))}{\bar \pi (a_t \vert s_t)} $$ 는 너무 작은 값이 되서 전혀 update가 이루어지지 않거나, 너무 큰 값이 되어 좋은 update가 이루어지지 않게 되는 것입니다. 이 문제를 해결하기 위해 여기서 objective를 다시 한번 바꿔보도록 하겠습니다. Trajectory의 distribution이 아니라 각 timestep의 (state, action) distribution으로 식을 재정의하는 것입니다.

$$ \nabla_{\theta} J(\theta) =  E_{\tau \sim \bar\pi(\tau)} [(\sum^{T}_{t=1} \cfrac{\pi_{\theta}(s_t, a_t)}{\bar \pi (s_t,a_t)} \log \pi_{\theta} (s_t \vert a_t)) (\sum^{T}_{t=1} r(s_t,a_t)]$$

$$ =  E_{\tau \sim \bar\pi(\tau)} [(\sum^{T}_{t=1} \cfrac{\pi_{\theta}(a_t \vert s_t) \pi_{\theta}(s_t)}{\bar \pi (a_t \vert s_t) \bar \pi (s_t)} \log \pi_{\theta} (a_t \vert s_t)) (\sum^{T}_{t=1} r(s_t,a_t)] $$

이제 $$ \prod^T_{t=1} $$은 사라졌지만 여전히 문제가 남아있습니다. $$ \pi_{\theta}(s_t) $$ 과 $$  \bar \pi (s_t) $$이 intractable하다는 것이지요. 하지만, Levine교수님은 사실 $$ \pi_{\theta}(s_t) $$ 과 $$  \bar \pi (s_t) $$을 지워버려도 된다고 하십니다. 왜 지워도 괜찮은지는 이후에 advanced policy gradient단원에서 다루어 주신다고 하시네요. 우선 off-policy방식으로 implementation을 해보고 싶다면,

$$ =  E_{\tau \sim \bar\pi(\tau)} [(\sum^{T}_{t=1} \cfrac{\pi_{\theta}(a_t \vert s_t)}{\bar \pi (a_t \vert s_t)} \log \pi_{\theta} (a_t \vert s_t)) (\sum^{T}_{t=1} r(s_t,a_t)] $$

방식으로 시도해보라고 말씀하십니다.


## Thoughts

이번 강의에서 다룰 내용은 여기까지입니다. 정리를 마치기전에 강의를 들으며 고민했던 몇가지 생각들이 있어 몇자 더 적어봅니다. 혹시 의견이 있으시다면 자유롭게 댓글로 알려주세요.

### Multivariate gaussian policy?

사실 continuous action space에서 action dimension의 크기가 n일 때, 모든 action이 independent하고 가정하는 이유는 상당히 practical한 이유 때문입니다. 학습해야할 covariance matrix $$ \sum $$의 파라미터 개수가 $$n$$에서 $$n^2$$로 늘어나기 때문이죠. 그럼에도 불구하고 우측 하단의 예제와 같이, action들의 correlation이 상당히 높은 문제들에 대해서는 독립에 대한 가정이 모델의 성능을 크게 저해할 수 있다는 우려가 남아있습니다.

<center><img src = "/assets/images/cs285/lecture5/mvn.png" width = "600"></center><br>

하지만 multivariate gaussian policy를 어떻게 학습하고 sampling을 할 수 있을지 고민을 해보니, 생각보다 쉬운 일이 아니었습니다. 이전과 같이 단순한 re-parameterization trick을 적용할 수 없었기 때문입니다. 우선, 이전과 유사하게 $$n$$ 차원의 random vector를 다음과 같이 sampling 합니다. $$ z \sim N(0,1) $$. 그렇다면, 저희의 action은 아래와 같이 생성되게 됩니다.

$$ a = \mu + \sqrt\sum \cdot z $$

그런데 이전과 달리 $$ \sum $$의 diagonal에만 value가 있는게 아니다보니, $$ a $$의 variance가 $$ \sum $$과 같지 않다는 문제가 발생하였습니다. 이를 해결하기 위해 선형대수학의 cholesky분해를 사용해보기로 하였습니다. $$\sum$$은 symmetry matrix이기 때문에, lower triangular matrix인 $$L$$로 cholesky분해를 진행할 수 있고, $$ \sum = LL^T $$로 나타내어 집니다. 이제 기존의 re-parameterization을 아래와 같이 바꿔보도록 하겠습니다. 

$$ a = \mu + L \cdot z $$

이렇게 바뀐 $$a$$의 variance는 $$ \sum $$과 동일할까요?

$$ Var(a) = E[a^2] - E[a]^2 $$

$$ = E[(\mu + L \cdot z)^2] - \mu^T\mu $$

$$ = E[\mu^T\mu] + E[\mu z^TL^T] + E[Lz\mu^T] + E[Lzz^TL^T] - \mu^T\mu $$

$$ = \mu^T\mu + \mu E[z^T]L^T + LE[z]\mu^T + LE[zz^T]L^T - \mu^T\mu $$

$$ = LL^T = \sum \,\,\,\,\,\,\,\,\,\,(E[z^T] = 0, E[zz^T] = 1) $$

이렇게 multivariate gaussian에서의 sampling문제는 해결되었습니다. 그렇다면, 실제로 model을 학습시킬때는 cholesky분해의 결과물인 $$L$$을 학습하면 되는 걸까요? 여기저기 github을 찾아봐도 애초에 multivaraite gaussian을 policy로 학습시킨 코드가 없는것이 아쉬웠습니다. 기회가 된다면 action간의 correlation이 높은 task를 찾아 실험을 해보고자 합니다. 

## Summing up

이번 강의에서 다룬 내용을 정리하자면 아래와 같습니다.

1. Introduction to REINFORCE algorithm
* Differentiation of policy gradient
* What is policy gradient?
* How can we implement this?

2. Two crucial problems and solutions for the policy gradient
* High variance -> casuality, baseline, normalization
* On policy -> importance sampling

<br>
여기까지 오시느라 정말 고생 많으셨습니다! 이번 강의는 내용이 정말 많았네요. 다음 시간에는 몇가지 재미있는 방법들을 통해서 policy gradient의 variance를 줄이는 방법들과 함께 actor-critic 알고리즘에 대해 소개하는 시간을 갖도록 하겠습니다. 감사합니다.


[andrewng_mvn]: https://www.youtube.com/watch?v=JjB58InuTqM
[karpathy_pong]: http://karpathy.github.io/2016/05/31/rl/
[ce_mle]: https://medium.com/konvergen/cross-entropy-and-maximum-likelihood-estimation-58942b52517a