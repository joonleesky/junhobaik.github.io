---
title:  "[PyTorch] Precautions for using Distributions"
date:   2020-03-01 12:04:36 +0900
tags:
  - PyTorch
---
This is a post that records the mistakes and precautions that I made while using the torch.distribution package.

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

### Categorical Distribution

*tf.nn.functional.softmax* is much slower and numerically unstable than *torch.nn.functional.log_softmax.*

```python
n = 5
d = 2

logits = torch.rand(n, d)
log_softmax = torch.nn.functional.log_softmax(logits)
probs = torch.exp(log_softmax)
p = Categorical(probs)
action = p.sample()
```

While performing classification without needs of explicit probability, just use *nn.functional.cross_entropy* since it is already combined with *log_softmax* and *nll_loss.*


### Diagonal Gaussian Distribution

There are 2 ways of constructing diagonal gaussian distribution.
1. torch.distributions.Normal
2. torch.distributions.MultivariateNormal

```python
n = 5
d = 2

std = torch.rand(d)
mu = torch.rand(n, d)
p1 = torch.distributions.Normal(mu, std)
p2 = torch.distributions.MultivariateNormal(mu, scale_tril=torch.diag(std))
x = p1.rsample()
assert p1.log_prob(x).sum(dim=1) != p2.log_prob(x)
```

However, even if the two results are eqaul, the first method is much faster since second method has been created to deal with the non-diagonal case as well.

When using *torch.distributions.MultivariateNormal*, use lower-triangular matrix rather than covariance_matrix if possible. 
Cholesky decomposition is performed if lower-triangular matrix is not given.