---
title:  "[CS 285] Lecture 7: Value Function Methods"
date:   2020-02-14 12:04:36 +0900
tags:
  - CS285
---
이번 포스팅에서는 actor가 없이 critic만으로 학습을 진행하는 **value function methods**들을 살펴보도록 하겠습니다. CS 285 강의를 기본으로 하되 흐름에 맞게 내용들을 추가하고 재배치했음을 밝힙니다. 부족한점에 대한 지적과 질문은 자유롭게 댓글로 남겨주세요.

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

## Excluding the actor from the actor-critic

우선 지금까지 살펴본 actor-critic 알고리즘을 high-level에서 살펴보도록 하겠습니다.

1. Simulation을 통해 policy $$ \pi $$에서의 sample들을 얻는다.
2. Critic을 통해 $$A^{\pi}(s,a)$$를 근사한다.
3. Policy gradient를 통해 actor를 update한다.

<br>
Actor-critic 알고리즘에서 학습에 주로 말썽을 일으키는 부분은 actor에 있었습니다. Policy gradient의 variance가 너무나 크기 때문에 학습을 시키는 것이 어려웠기 때문이죠. 그렇다면 문제의 원인인 actor를 없앨 수는 없을까요? Actor는 actor-critic에서 policy $$\pi$$의 역할을 수행하기 때문에, actor를 없애기 위해선 critic만으로 $$\pi$$를 나타낼 수 있어야 합니다. Critic만으로 $$\pi$$를 나타내는 방법은 정말 간단합니다. 주어진 state $$s_t$$에서 critic이 가장 좋다고 생각하는 방향으로 deterministic하게 이동하는 것입니다.

$$ \pi(a_t \vert s_t) = \begin{cases} 1 & \text{if } a = arg\, max_{a_t} A^{\pi}(s,a) \\ 0 & \text{otherwise} \\ \end{cases}$$

그렇다면, 위의 actor-critic 알고리즘은 아래와 같이 바뀔 수가 있습니다.

1. Simulation을 통해 policy $$ \pi $$에서의 sample들을 얻는다.
2. Critic을 통해 $$A^{\pi}(s,a)$$를 근사한다.
3. $$A^{\pi}(s,a)$$를 통해 policy를 update한다. $$ \pi(a_t \vert s_t) = arg\, max_{a_t} A^{\pi}(s_t,a_t) $$

<br>

따라서, 이제는 더 이상 policy gradient를 사용하지 않기 때문에, high varaiance problem에 더 이상 시달리지 않아도 괜찮습니다. 하지만, actor가 사라지면서 잃게된 것도 있습니다. 과거에는 actor를 통해 policy를 stochastic하게 나타낼 수 있었지만, 지금은 과거와는 달리 policy가 언제나 deterministic하게 표현되게 됩니다. 그렇다면, policy가 deterministic할 때 발생하는 문제점은 없을까요?

Levine교수님은 주어진 환경이 fully-observable mdp라면 언제나 1개 이상의 optimal한 deterministic policy가 존재하기 때문에 수렴성에 문제가 존재하지 않지만, partially-observable한 환경에선 deterministic policy가 여러가지 문제점들을 야기할 수 있다고 말씀하십니다. 본격적으로 알고리즘들을 다루기에 앞서, 몇가지 문제점들을 가볍게 살펴보고 넘어가도록 하겠습니다.

## Problems of the deterministic policy 

### Rock-paper-scissors
 
첫번째 예제는 가위바위보 입니다. 저희의 agent가 언제나 deterministic하게 가위, 바위, 보 중에 한개를 낸다면, 이런 agent를 상대로 이기는 것은 당연히도 너무나 쉬운 일입니다. 가위바위보에서의 optimal policy는 가위, 바위, 보를 모두 $$\frac{1}{3}$$의 확률로 랜덤하게 내는 stochastic policy 입니다.

### Aliased gridworld

아래 그림과 같은 미로에서 탈출해야 하는 agent를 생각해보도록 하죠. 아래의 환경은 agent가 인접한 state만을 볼 수 있는 partially observable mdp입니다.  

<center><img src = "/assets/images/cs285/lecture7/aliased_gridworld.png" width = "200"></center>
<div style = "margin-bottom:12px"></div>
<center>Figure 1: The agent cannot distinguish between the grey states.</center><br>

Agent의 입장에선 두개의 grey state의 인접한 state들이 동일하기 때문에, 두 state들을 구분할 수 없게 됩니다. 따라서, 주어진 policy가 deterministic하다면 agent는 grey state에서 언제나 왼쪽으로 가거나, 오른쪽으로 가는 것을 선택할 수 밖에 없습니다.  

<center><img src = "/assets/images/cs285/lecture7/aliased_gridworld2.png" width = "200"></center>
<div style = "margin-bottom:12px"></div>
<center>Figure 2: agent cannot escape with the deterministic policy</center><br>

이런 deterministic policy에서는 언제나 오른쪽 위의 두개의 state, 또는 왼쪽 위의 두개의 state에서는 탈출할 수 없게 됩니다. 하지만, agent가 stochastic policy를 활용한다면 어떨까요? 두개의 grey state에서 오른쪽으로 갈 확률과 왼쪽으로 갈 확률이 절반씩 있는 것입니다. 

<center><img src = "/assets/images/cs285/lecture7/aliased_gridworld3.png" width = "200"></center>
<div style = "margin-bottom:12px"></div>
<center>Figure 3: agent can escape the maze in a few timesteps</center><br>

위와 같은 stochastic policy를 사용한다면, agent는 grey state를 시간이 지남에 따라 빠져나올 수 있게 됩니다. 정리하자면, deterministic policy는 주어진 environment가 adversarial하거나 state-representation들이 markov하게 나타나지 않을때 stochastic policy에 비해서 문제가 나타날 수 있습니다. 

하지만, 우선은 determinsitic policy가 문제를 일으키는 환경들은 뒤로하고, fully-observable한 단순한 환경들부터 해결해보도록 하겠습니다. 


## Value-based methods in Known MDP

첫번째로 해결해볼 환경은 fully-observable한 환경임과 동시에, transition probability $$p(s^{\prime} \vert s, a) $$가 알려져 있는 Known MDP문제입니다. Critic만을 이용한 강화학습 알고리즘을 다시 써보면 아래와 같이 나타낼 수 있습니다.

1. Get sample through simulation with policy $$ \pi $$.
2. Evaluate $$A^{\pi}(s,a)$$
3. Update the policy. $$ \pi^{\prime} \leftarrow arg\, max_{a} A^{\pi}(s,a) $$

(2)의 '$$A^{\pi}(s,a)$$를 evaluate 하기 위해 $$A^{\pi}(s,a)$$의 수식을 좀 더 자세히 들여다보도록 하겠습니다.

$$A^{\pi}(s,a) = r(s,a) + \gamma E_{a \sim \pi(s), \, s^{\prime} \sim p(s^{\prime} \vert s, a)}[V^\pi(s^{\prime})]-V^\pi(s)$$

주어진 환경의 $$p(s^{\prime} \vert s, a) $$가 알려져 있고, action역시 deterministic하기 때문에 $$V^\pi(s)$$를 알 수 있다면 expectation을 계산하여 $$A^{\pi}(s,a)$$를 쉽게 구할 수 있게됩니다. 따라서, $$A^{\pi}(s,a)$$를 평가하는 문제는 $$V^\pi(s)$$를 평가하는 문제로 단순화 시킬 수 있게 됩니다. 그럼 $$V^\pi(s)$$는 어떻게 평가할 수 있을까요? 이번엔 $$V^{\pi}(s)$$의 수식을 좀 더 들여다보도록 하죠.

$$V^{\pi}(s) \leftarrow r(s,\pi(s)) + \gamma E_{a \sim \pi(s), s^{\prime} \sim p(s^{\prime} \vert s, a)}[V^\pi(s^{\prime})] $$

$$A^{\pi}(s,a)$$과 마찬가지로 우리는 expectation을 쉽게 계산할 수 있고, 현재 가지고 있는 $$V^{\pi}(s)$$를 통해 점진적으로 $$V(s)$$값을 개선해 나가는 것입니다.

### Policy iteartion

위의 내용들을 종합한 알고리즘이 바로 policy iteration 알고리즘입니다. 우선 모든 state에 대하여 $$V(s)$$값을 0으로 초기화한 이후, $$V^\pi(s)$$를 평가하는 policy evaluation과 $$ A^{\pi}(s,a)$$를 바탕으로 policy를 수정하는 policy improvement를 반복적으로 수행하는 것입니다.

1. Evaluate $$V^\pi(s)$$ for all of the states. $$\,V^{\pi}(s) \leftarrow r(s,\pi(s)) + \gamma E_{a \sim \pi(s), s^{\prime} \sim p(s^{\prime} \vert s, a)}[V^\pi(s^{\prime})] $$
2. Update the policy. $$ \pi^{\prime} \leftarrow arg\, max_{a} A^{\pi}(s,a) $$.

### Value iteration

Policy iteration 알고리즘에 의문이 하나 있습니다. 저희의 목표는 optimal policy $$ \pi^{*}$$를 찾는 것인데, optimal value function $$V^*$$에 대하여 $$ \pi^{*} = arg\, max_{a} A^{*}(s,a) $$를 만족한다는 것을 이미 알고 있습니다. 그렇다면, 이전과 달리 policy를 skip하고 $$V^*$$를 찾는 것에만 집중해도 optimal policy $$ \pi^{*}$$를 찾을 수 있지 않을까요?

1. Evaluate $$Q(s,a)$$ for all of the states. $$\,\, Q(s,a) \leftarrow r(s,a) + \gamma E_{s^{\prime} \sim p(s^{\prime} \vert s, a)}[V(s^{\prime})] $$
2. Update $$V(s)$$. $$\,\, V(s) \leftarrow  max_{a} Q(s,a) $$

Policy iteration과 달리 주어진 policy가 없기 때문에, 주어진 state의 모든 action에 대한 value값인 $$ Q(s,a) $$를 구하고 이를 통해 $$ V(s) $$를 update하는 과정을 반복하는 것입니다. 만약 이 과정을 반복해서 $$ V $$가 $$V^*$$로 converge하는 것이 보장된다면, $$ \pi^{*} = arg\, max_{a} A^{*}(s,a) $$를 통하여 optimal policy역시 추출할 수 있게됩니다. 따라서, 다음 section에선 value iteration 알고리즘이 정말 $$V^*$$로 수렴하는지 증명을 해보도록 하겠습니다.

### Convergence of value iteration

먼저, value iteration의 알고리즘을 operator $$ \mathcal{B} $$ 를 통해 정의해 보도록 하겠습니다.

$$ \mathcal{B}: \mathcal{B}V =  max_{a} (r_a + \gamma \,T_{a} V ) $$  

$$r_a: $$ 모든 state에서 action a를 취할 때의 reward를 쌓아올린 vector<br> 
$$T_{a}$$: $$T_{a,i,j} = p(s^{\prime}=i \vert s = j, a)$$ 를 만족하는 transition matrix

<img src = "/assets/images/cs285/lecture7/vi.png" width = "300">
<div style = "margin-bottom:12px"></div>


다음 optimal value $$V^*$$와 $$ \mathcal{B}$$의 관계를 살펴보도록 하겠습니다.

$$ V^{*}(s) =  max_{a} r(s,a) + \gamma \,E [V^{*}(s^\prime)] $$

$$ V^{*} = \mathcal{B}V^* $$


이제 $$\infty$$-norm 의 공간에서 임의의 두 점 $$V_1$$과 $$V_2$$에 $$ \mathcal{B} $$를 진행했을 때의 거리의 변화를 계산해보도록 하겠습니다.

$$
\begin{align*}
 \vert \vert \mathcal{B}V_1 - \mathcal{B}V_2  \vert \vert_{\infty} &= \vert \vert \gamma \,T (V_1 - V_2)  \vert \vert_{\infty} \\[8pt]
 & \leq \gamma \,\,\vert \vert \, T  \vert \vert_{\infty} \,\,\vert \vert \,(V_1 - V_2)  \vert \vert_{\infty} \,\,\,\, \text{inequality holds since T is row-stochastic} \\[8pt]
 & \leq \gamma \,\,\vert \vert \,(V_1 - V_2)  \vert \vert_{\infty} \\[8pt]
\end{align*}
$$

따라서, $$ \mathcal{B} $$는 $$ \infty$$-norm의 공간에서 임의의 $$V$$들에 대해 $$\gamma \,$$만큼 거리를 축소시키는 contraction 함수를 만족하게 됩니다. <br> $$ \rightarrow $$ *contraction 함수란 주어진 거리 공간에서 정의된 두 점 사이의 거리를 축소시키는 함수를 의미합니다*

이제 $$V_2$$대신 $$V^*$$를 위 식에 대입해보도록 하겠습니다.

$$ \vert \vert \mathcal{B}V_1 - \mathcal{B}V^*  \vert \vert_{\infty} = \vert \vert \mathcal{B}V_1 - V^*  \vert \vert_{\infty} \leq \gamma \,\,\vert \vert \,(V_1 - V^*)  \vert \vert_{\infty} $$ 

따라서, value iteration을 한번 수행할때마다 $$V$$는 $$\infty$$-norm에서 최소 $$\gamma \,$$만큼 $$V^*$$에 가까워지게 됩니다. $$\gamma \,$$는 $$[0,1]$$사이의 값이기 때문에, value iteration을 반복해서 적용한다면 $$V$$는 $$V^*$$에 수렴함이 증명되었습니다.

### Fitted value iteration

이제 value iteration이 optimal value function을 찾아낼 수 있음을 알게되었지만, 현실의 문제들에 value iteration을 적용하는 것은 쉬운 일이 아닙니다.

<center><img src = "/assets/images/cs285/lecture7/large_state.png" width = "320"></center><br>

그림과 같이 state가 $$ 200 \times 200 $$의 image라면, state의 개수는 $$(255^3)^{200 \times 200}$$에 육박합니다. 모든 state에 대하여 value를 계산하는 것은 불가능할 뿐만이 아니라, 저만한 크기의 table을 구성하는 것도 현실적이지 않습니다. 

이런 문제점을 해결하기 위해 이전과 같이 neural network와 같은 function approximator를 사용할 수 있습니다. State를 value로 mapping하는 function approximator $$V{\phi}(s)$$ 를 학습해가며 value iteration을 진행하는 것을 우리는 fitted value iteration이라고 부릅니다.

1. set $$ \,\, Q(s_i,a_i) \leftarrow r(s_i,a_i) + \gamma E_{s^{\prime} \sim p(s^{\prime} \vert s, a)}[V_{\phi}(s^{\prime})] $$
2. set $$ \,\, y_i \leftarrow  max_{a_i} Q(s_i,a_i) $$
3. train neural network.  $$ \phi \leftarrow arg\,min_{\phi} \frac{1}{2}\sum_i \vert \vert V{\phi}(s_i) - y_i  \vert \vert^2 $$

하지만, 다시 확인해봐야 할 것이 생겼습니다. Neural network와 같은 function approximator를 사용하더라도 이전과 같이 optimal value로 수렴할 수 있을까요?

### Convergence of fitted value iteration

이전과 같이 fitted value iteration의 알고리즘을 operator $$ \Pi , \mathcal{B} $$ 를 통해 정의해 보도록 하겠습니다.

$$ \mathcal{B}: \mathcal{B}V =  max_{a} (r_a + \gamma \,T_{a} V ) $$  
$$ \Pi: \Pi V =  arg \, min_{V^{\prime}\in \Omega} \frac{1}{2}\sum \vert \vert V^{\prime}(s) - V(s)  \vert \vert^2 $$  

<img src = "/assets/images/cs285/lecture7/fvi.png" width = "400">
<div style = "margin-bottom:12px"></div>

여기서 $$\Pi$$ operator는 저희의 function approximator의 subspace $$\Omega$$에 대한 projection operator와 같게 됩니다. 따라서, fitted value iteration을 $$\Pi \mathcal{B} V $$를 통해 풀어 설명하면 $$\mathcal{B}$$를 통해 $$\infty -norm$$에서 $$V$$를 $$V^*$$에 가깝게 이동하고, $$\Omega$$의 subspace에 projection을 가하는 것과 동일하게 생각할 수 있습니다.

<center><img src = "/assets/images/cs285/lecture7/fvi_pic.png" width = "400"></center><br>

따라서, $$\Pi \mathcal{B}$$는 어떠한 종류의 contraction도 만족하지 못하게 되고, 자연스럽게 $$V^*$$로의 convergence역시 보장할 수 없게 됩니다. 하지만, neural network의 subspace인 $$\Omega$$는 온갖 종류의 function의 형태를 지닐 수 있기 때문에 실제 적용에선 optimal value에 가깝게 수렴하곤 합니다.

간단하게 정리해보자면 저희는 transition probability가 알려져 있는 known mdp에서 dynamic programming 방법을 통해 optimal policy를 찾아내는 2가지 방법을 살펴보았습니다.

1. Policy iteration
2. Value iteration

이후, Dynamic programming을 현실 문제에 적용하기에는 state의 크기가 너무나 크기 때문에 function approximator를 활용한 fitted value iteration역시 살펴보았습니다.

## Value-based methods in Unknown MDP

하지만, 우리가 강화학습으로 해결하고자 하는 대부분의 환경들은 transition dynamics를 모르는 Unknown MDP입니다. 이러한 환경에 fitted value iteration 알고리즘을 적용할 순 없을까요?

1. set $$ \,\, y_i \leftarrow  max_{a_i} r(s_i,a_i) + \gamma E_{s^{\prime} \sim p(s^{\prime} \vert s, a)}[V_{\phi}(s^{\prime})] $$
2. train neural network.  $$ \phi \leftarrow arg\,min_{\phi} \frac{1}{2}\sum_i \vert \vert V{\phi}(s_i) - y_i  \vert \vert^2 $$

위 식에서 문제가 되는 부분은 바로 $$max_{a}$$입니다. Transition dynamics를 모르는 환경에서는 어떠한 action이 가장 큰 $$ r(s_i,a_i) + \gamma E_{s^{\prime} \sim p(s^{\prime} \vert s, a)}[V_{\phi}(s^{\prime})])$$를 가져올지 알 수 없기 때문입니다. 

### Fitted Q-iteration

그렇다면, 각 action에 대한 결과들을 알 수 없다면, 각 action에 대한 기댓값을 학습하는 것은 어떨까요? $$ Q_{\phi}(s,a)  = r(s,a) + \gamma E[V_{\phi}(s^{\prime})]$$라는 것을 활용하여 $$ V_{\phi}(s) $$대신 $$ Q_{\phi}(s,a)$$를 학습하는 것입니다. 

1. set $$ \,\, y_i \leftarrow  r(s_i,a_i) + \gamma E_{s^{\prime} \sim p(s^{\prime} \vert s, a)}[V_{\phi}(s^{\prime})] $$
2. train neural network.  $$ \phi \leftarrow arg\,min_{\phi} \frac{1}{2}\sum_i \vert \vert Q{\phi}(s_i, a_i) - y_i  \vert \vert^2 $$

그렇다면 $$ V_{\phi}$$를 모르는 상태에서 $$E_{s^{\prime} \sim p(s^{\prime} \vert s, a)}[V_{\phi}(s^{\prime})]$$를 어떻게 알 수 있을까요?

$$E_{s^{\prime} \sim p(s^{\prime} \vert s, a)}[V_{\phi}(s^{\prime})] \approx V_{\phi}(s^{\prime}) =  max_{a^{\prime}}Q_{\phi}(s_i^{\prime}, a_i^{\prime}) $$

따라서, 저희는 $$ Q_{\phi}(s,a) $$를 통하여 fitted value iteration에서의 max 부분의 문제점을 해결할 수 있습니다.

하지만, 이전과는 달리 dynamic programming이 아닌 simulation을 통해서 값을 추정해야 하기 때문에 dataset을 모으는 과정 역시 필요합니다. 알고리즘을 종합하면 아래와 같습니다.

<center><img src = "/assets/images/cs285/lecture7/ffq.png" width = "450"></center><br>

### Off-policy

Policy-gradient 알고리즘을 돌아보면 현재의 policy $$ \pi $$에 대한 trajectory의 expecation을 구해왔기 때문에 과거의 policy에서 sample된 data는 사용할 수 없는 on-policy 알고리즘 이였습니다. 또한, policy gradient를 off-policy 형태로 사용하기 위해서는 importance sampling등의 trick이 필요했습니다. 하지만, Q-iteration에서 $$ Q_{\phi}(s,a) $$를 학습하는 과정은 $$ \pi $$와 전혀 무관하기 때문에 어떠한 policy에서 생성된 data도 사용할 수 있는 off-policy 알고리즘이 됩니다.

Q-iteration이 online 방식으로 데이터를 모으기와 모델 학습이 동시에 이루어지는 경우를 **Q-learning** 알고리즘이라고 합니다.

<center><img src = "/assets/images/cs285/lecture7/qlearn.png" width = "430"></center><br>

## Summing up

이번 강의에서 다룬 내용을 정리하자면 아래와 같습니다.

1. From policy based to value based methods
2. Value-based methods in Known MDP
* Policy iteration
* Value iteration
* Fitted value iteration
3. Value-based methods in Unknown MDP
* Fitted Q-iteration
4. Theory
* Convergence of value iteration
* Convergence of fitted value iteration


<br>
이번 강의에서는 가장 대표적인 value-based 방식의 알고리즘들을 살펴보았습니다. 하지만, fitted Q-learning을 atari와 같은 실제 게임에 적용할 때는 좋은 function approximator를 학습시키는 것이 쉬운 일이 아닙니다. 다음 강의에선 다양한 trick들을 이용해서 어떻게 현실의 문제들에 value-based 알고리즘들을 적용할 수 있는지 살펴보도록 하겠습니다. 감사합니다.


[sutton]: http://www.incompleteideas.net/book/the-book-2nd.html
[andrewng_mvn]: https://www.youtube.com/watch?v=JjB58InuTqM
[karpathy_pong]: http://karpathy.github.io/2016/05/31/rl/
[ce_mle]: https://medium.com/konvergen/cross-entropy-and-maximum-likelihood-estimation-58942b52517a