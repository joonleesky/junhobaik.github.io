---
title:  "(작성중)[CS 285] Lecture 7: Value Function Methods"
date:   2020-02-14 12:04:36 +0900
tags:
  - CS285
---
이번 포스팅에서는 actor가 없이 critic만으로 학습을 진행하는 **value function methods**들을 살펴보도록 하겠습니다. CS 285 강의를 기본으로 하되 흐름에 맞게 내용들을 추가하고 재배치했음을 밝힙니다. 부족한점에 대한 지적과 질문은 자유롭게 댓글로 남겨주세요.

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

## Excluding the actor from the actor-critic

우선 지금까지 살펴본 actor-critic 알고리즘을 high-level에서 살펴보도록 하죠.

1. Simulation을 통해 policy $$ \pi $$에서의 sample들을 얻는다.
2. Critic을 통해 $$A^{\pi}(s,a)$$를 근사한다.
3. Gradient ascent를 통해 actor를 update한다.

<br>
Actor-critic 알고리즘에서 학습에 말썽을 일으키는 부분은 언제나 actor에 있었습니다. Policy gradient의 variance가 너무나 크기 때문에 학습이 어려웠기 때문이죠. 그렇다면 문제의 원인인 actor를 없앨 수는 없을까요? Actor는 actor-critic에서 policy $$\pi$$의 역할을 수행하기 때문에, actor를 없애기 위해선 critic만으로 $$\pi$$를 나타낼 수 있어야 합니다. Critic만으로 $$\pi$$를 나타내는 방법은 정말 간단합니다. 주어진 state $$s_t$$에서 critic이 가장 좋다고 생각하는 방향으로 deterministic하게 이동하는 것입니다.

$$ \pi(a_t \vert s_t) = \begin{cases} 1 & \text{if } a = arg\, max_{a_t} A^{\pi}(s,a) \\ 0 & \text{otherwise} \\ \end{cases}$$

그렇다면 과거에 actor로 표현됬던 policy를 critic에 의존해서 위와 같이 나타내면 문제점은 없을까요? 주어진 상황에서 행동들이 stochastic하게 결정되는 것이 아니라, deterministic하게 결정되어도 괜찮은 것일까요?

Levine교수님은 주어진 환경이 fully-observable mdp라면 언제나 1개 이상의 optimal한 deterministic policy가 존재하기 때문에 괜찮다고 말씀하시지만, 환경이 partially-observable하다면 deterministic policy는 여러가지 문제점들을 야기할 수 있습니다. 그 여러가지 문제점들을 한번 가볍게 살펴보고 넘어가도록 하겠습니다.

## Problems of the deterministic policy 

### Rock-paper-scissors
 
첫번째 예제는 가위바위보 입니다. 저희의 agent가 언제나 deterministic하게 가위, 바위, 보 중에 한개를 낸다면, 이런 agent를 상대로 이기는 것은 당연히도 너무나 쉬운 일입니다. 가위바위보에서의 optimal policy는 가위, 바위, 보를 모두 $$\frac{1}{3}$$의 확률로 랜덤하게 내는 stochastic policy 입니다.

### Aliased gridworld

아래 그림과 같은 미로에서 탈출해야 하는 agent를 생각해보도록 하죠. 아래의 환경은 agent가 인접한 state만을 볼 수 있는 partially observable mdp입니다.  

<center><img src = "/assets/images/cs285/lecture7/aliased_gridworld.png" width = "200"></center>
<div style = "margin-bottom:12px"></div>
<center>Figure 1: The agent cannot distinguish between the grey states.</center><br>

Agent의 입장에선 두개의 grey state의 인접한 state들이 동일하기 때문에, 두 state들을 구분할 수 없게 됩니다. 따라서, 주어진 policy가 deterministic하다면 agent는 grey state에서 언제나 왼쪽으로 가거나, 오른쪽으로 가는 것을 선택할 수 밖에 없습니다.  

<center><img src = "/assets/images/cs285/lecture7/aliased_gridworld2.png" width = "200"></center>
<div style = "margin-bottom:12px"></div>
<center>Figure 2: agent cannot escape with the deterministic policy</center><br>

이런 deterministic policy에서는 언제나 오른쪽 위의 두개의 state, 또는 왼쪽 위의 두개의 state에서는 탈출할 수 없게 됩니다. 하지만, agent가 stochastic policy를 활용한다면 어떨까요? 두개의 grey state에서 오른쪽으로 갈 확률과 왼쪽으로 갈 확률이 절반씩 있는 것입니다. 

<center><img src = "/assets/images/cs285/lecture7/aliased_gridworld3.png" width = "200"></center>
<div style = "margin-bottom:12px"></div>
<center>Figure 3: agent can escape the maze in a few timesteps</center><br>

위와 같은 stochastic policy를 사용한다면, agent는 grey state를 시간이 지남에 따라 빠져나올 수 있게 됩니다. 정리하자면, deterministic policy는 주어진 environment가 adversarial하거나 state-representation들이 markov하게 나타나지 않을때 stochastic policy에 비해서 문제가 나타날 수 있습니다. 

하지만 우선은 문제를 단순화시켜 주어진 환경이 fully-observable mdp를 만족한다고 전제하고, critic만으로 학습을 진행하는 방법들을 살펴보도록 하겠습니다.

## Dynamic programming

기존의 actor-critic 알고리즘을 critic만으로 재구성한다면 아래와 같이 나타낼수 있습니다. 

1. Evaluate $$A^{\pi}(s,a)$$
2. Update the policy. $$ \pi^{\prime} \leftarrow arg\, max_{a} A^{\pi}(s,a) $$

그렇다면, critic을 이용하여 $$A^{\pi}(s,a)$$를 어떻게 평가할 수 있을까요? 우선 주어진 환경의 transition probability $$p(s^{\prime} \vert s, a) $$가 알려져 있고, action이 discrete한 단순한 문제부터 생각해보도록 하겠습니다. $$A^{\pi}(s,a)$$의 수식을 살펴보면 아래와 같습니다. 

$$A^{\pi}(s,a) = r(s,a) + \gamma E_{a \sim \pi(s), \, s^{\prime} \sim p(s^{\prime} \vert s, a)}[V^\pi(s^{\prime})]-V^\pi(s)$$

주어진 환경의 $$p(s^{\prime} \vert s, a) $$가 알려져 있다고 가정했기 때문에, $$V^\pi(s)$$를 알 수 있다면 expectation을 실제로 계산하여 $$A^{\pi}(s,a)$$ 역시 계산할 수 있게 되고,  따라서, $$A^{\pi}(s,a)$$를 평가하는 문제는 $$V^\pi(s)$$를 평가하는 문제로 단순화 시킬 수 있습니다. 그럼 $$V^\pi(s)$$는 어떻게 구할 수 있을까요? Dynamic programming을 통해 $$V^\pi(s)$$를 저장하면서 temporal-difference 방식으로 개선해 나가는 것입니다.

$$V^{\pi}(s) \leftarrow r(s,\pi(s)) + \gamma E_{a \sim \pi(s), s^{\prime} \sim p(s^{\prime} \vert s, a)}[V^\pi(s^{\prime})] $$

### Policy iteartion

위의 내용들을 종합한 알고리즘이 바로 policy iteration 알고리즘입니다. 우선 모든 state에 대하여 $$V(s)$$값을 0으로 초기화한 이후, $$V^\pi(s)$$를 평가하는 policy evaluation과 $$ A^{\pi}(s,a)$$를 바탕으로 policy를 수정하는 policy improvement를 반복적으로 수행하는 것입니다.

1. Evaluate $$V^\pi(s)$$ for all of the states. $$\,V^{\pi}(s) \leftarrow r(s,\pi(s)) + \gamma E_{a \sim \pi(s), s^{\prime} \sim p(s^{\prime} \vert s, a)}[V^\pi(s^{\prime})] $$
2. Update the policy. $$ \pi^{\prime} \leftarrow arg\, max_{a} A^{\pi}(s,a) $$.

<br>
 Richard sutton 교수님은 $$ \pi $$가 더 이상 변하지 않을때까지 policy iteration알고리즘을 반복한다면, value와 policy모두 optimal하게 수렴함을 [Reinforcement Learning: An Introduction][sutton]에 증명해 두었습니다. 

### Value iteration

사실 저희는 optimal value function $$V^*$$에 대하여 $$ \pi^{*} = arg\, max_{a} A^{*}(s,a) $$를 만족한다는 것을 알고 있습니다. 그렇다면, 이전과 달리 policy를 update하지 말고 $$V$$가 $$V^*$$가 될때까지 update하는 것은 어떨까요?

1. Evaluate $$Q(s,a)$$ for all of the states. $$\,\, Q(s,a) \leftarrow r(s,a) + \gamma E_{s^{\prime} \sim p(s^{\prime} \vert s, a)}[V(s^{\prime})] $$
2. Update $$V(s)$$. $$\,\, V(s) \leftarrow  max_{a} Q(s,a) $$

만약 이 과정을 반복해서 $$ V $$가 $$V^*$$로 converge하게 된다면, 이전과 같이 $$ \pi^{*} = arg\, max_{a} A^{*}(s,a) $$를 통하여 optimal policy를 추출할 수 있습니다. 따라서, value iteration이 optimal policy를 찾아낼 수 있는지 확인해보기 위해 $$V^*$$로의 수렴성에 대한 증명을 해보도록 하겠습니다.

### Convergence of value iteration

먼저, value iteration의 과정을 하나의 operator $$ \mathcal{B} $$ 로 정의하도록 하겠습니다.

$$ \mathcal{B}: \mathcal{B}V =  max_{a} (r_a + \gamma \,T_{a} V ) $$  

$$r_a: $$ 모든 state에서 action a를 취할 때의 reward를 쌓아올린 vector<br> 
$$T_{a}$$: $$T_{a,i,j} = p(s^{\prime}=i \vert s = j, a)$$ 를 만족하는 transition matrix

다음 optimal value $$V^*$$와 $$ \mathcal{B}$$의 관계를 살펴보도록 하겠습니다.

$$ V^{*}(s) =  max_{a} r(s,a) + \gamma \,E [V^{*}(s^\prime)] $$

$$ V^{*} = \mathcal{B}V^* $$


이제 $$\infty$$-norm 의 공간에서 임의의 두 점 $$V_1$$과 $$V_2$$에 $$ \mathcal{B} $$를 진행했을 때의 거리의 변화를 계산해보도록 하겠습니다.

$$
\begin{align*}
 \vert \vert \mathcal{B}V_1 - \mathcal{B}V_2  \vert \vert_{\infty} &= \vert \vert \gamma \,T (V_1 - V_2)  \vert \vert_{\infty} \\[8pt]
 & \leq \gamma \,\,\vert \vert \, T  \vert \vert_{\infty} \,\,\vert \vert \,(V_1 - V_2)  \vert \vert_{\infty} \,\,\,\, \text{inequality holds since T is row-stochastic} \\[8pt]
 & \leq \gamma \,\,\vert \vert \,(V_1 - V_2)  \vert \vert_{\infty} \\[8pt]
\end{align*}
$$

따라서, $$ \mathcal{B} $$는 $$ \infty$$-norm의 공간에서 임의의 $$V$$들에 대해 $$\gamma \,$$만큼 거리를 축소시키는 contraction 함수를 만족하게 됩니다. <br> $$ \rightarrow $$ *contraction 함수란 주어진 거리 공간에서 정의된 두 점 사이의 거리를 축소시키는 함수를 의미합니다*

이제 임의의 $$V$$인 $$V_2$$대신 $$V^*$$를 위 식에 대입해보도록 하겠습니다.

$$ \vert \vert \mathcal{B}V_1 - \mathcal{B}V^*  \vert \vert_{\infty} = \vert \vert \mathcal{B}V_1 - V^*  \vert \vert_{\infty} \leq \gamma \,\,\vert \vert \,(V_1 - V^*)  \vert \vert_{\infty} $$ 

따라서, value iteration을 한번 수행할때마다 $$V$$는 $$\infty$$-norm에서 최소 $$\gamma \,$$만큼 $$V^*$$에 가까워지게 됩니다. $$\gamma \,$$는 $$[0,1]$$사이의 값이기 때문에, value iteration을 반복해서 적용한다면 $$V$$는 $$V^*$$에 수렴함이 증명되었습니다.

### Fitted value iteration

이제 value iteration이 optimal value function을 찾아낼 수 있음을 알게되었지만, 현실의 문제들에 value iteration을 적용하는 것은 쉬운 일이 아닙니다.

<center><img src = "/assets/images/cs285/lecture7/large_state.png" width = "320"></center><br>

그림과 같이 state가 $$ 200 \times 200 $$의 image라면, state의 개수는 $$(255^3)^{200 \times 200}$$에 육박합니다. 모든 state에 대하여 value를 계산하는 것은 불가능할 뿐만이 아니라, 저만한 크기의 table을 구성하는 것도 현실적이지 않습니다. 

이런 문제점을 해결하기 위해 이전과 같이 neural network와 같은 function approximator를 사용할 수 있습니다. State를 value로 mapping하는 function approximator $$V{\phi}(s)$$ 를 학습해가며 value iteration을 진행하는 것을 우리는 fitted value iteration이라고 부릅니다.

1. set $$ \,\, Q(s_i,a_i) \leftarrow r(s_i,a_i) + \gamma E_{s^{\prime} \sim p(s^{\prime} \vert s, a)}[V_{\phi}(s^{\prime})] $$
2. set $$ \,\, y_i \leftarrow  max_{a_i} Q(s_i,a_i) $$
3. train neural network.  $$ \phi \leftarrow arg\,min_{\phi} \frac{1}{2}\sum_i \vert \vert V{\phi}(s_i) - y_i  \vert \vert^2 $$

하지만, 다시 확인해봐야 할 것이 생겼습니다. Neural network와 같은 function approximator를 사용하더라도 이전과 같이 optimal value로 수렴할 수 있을까요?

### Convergence of fitted value iteration

(On progress)

간단하게 정리해보자면 저희는 transition probability를 알고있는 known mdp에서 explicit한 policy인 actor 없이 학습하는 알고리즘 2개를 살펴보았습니다.
1. Policy iteration
2. Value iteration


## Fitted Q-iteration




### Off-policy

### Online Q-learning







[sutton]: http://www.incompleteideas.net/book/the-book-2nd.html
[andrewng_mvn]: https://www.youtube.com/watch?v=JjB58InuTqM
[karpathy_pong]: http://karpathy.github.io/2016/05/31/rl/
[ce_mle]: https://medium.com/konvergen/cross-entropy-and-maximum-likelihood-estimation-58942b52517a