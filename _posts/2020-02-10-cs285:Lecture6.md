---
title:  "(On progress)[CS 285] Lecture 6: Actor-Critic Algorithms"
date:   2020-02-10 12:04:36 +0900
tags:
  - CS285
---
이번 포스팅에서는 policy gradient의 variance를 어떻게 줄일 수 있는지 살펴보고 **actor-critic** 알고리즘에 대해 소개하는 시간을 갖도록 하겠습니다. CS 285 강의를 기본으로 하되 흐름에 맞게 내용들을 추가하고 재배치했음을 밝힙니다. 부족한점에 대한 지적과 질문은 자유롭게 댓글로 남겨주세요.

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

## Policy evaluation

우선 casuality와 baseline을 도입한 policy gradient의 식을 다시 살펴보겠습니다.

$$ 
\begin{align*}
\nabla_{\theta} J(\theta) &=  E_{\tau \sim p_{\theta} (\tau)} [ (\sum^{T}_{t=1} \nabla_{\theta} \log \pi_{\theta} (a_t \vert s_t)) (\sum^{T}_{t^\prime=t} r(s_t^\prime,a_t^\prime) - b)] \\
& \approx \sum^{N}_{i=1} \,\, (\sum^{T}_{t=1} \nabla_{\theta} \log \pi_{\theta} (a_t \vert s_t)) (\sum^{T}_{t^\prime=t} r(s_t^\prime,a_t^\prime) - b)
\end{align*}
$$

비록 기존의 policy gradient의 식에 비해서는 많이 발전했지만, 여전히 variance는 너무나 높습니다. 따라서 우리는 위의 수식을 보다 정교하게 estimate할 수 있는 방법을 고민해보아야 합니다. 

### Improving the baseline

지난 시간에 simple한 baseline으로는 단순히 trajectory들의 average reward를 사용한다고 말씀드렸습니다. 그렇다면, casuality를 반영한 average reward를 보다 정교하게 estimate해보는건 어떨까요? Baseline으로 average reward를 estimate하는 $$V^{\pi}_{\psi}(s_t) $$를 통하여 variance를 낮추는 것입니다.

$$ \nabla_{\theta} J(\theta) \approx \sum^{N}_{i=1} \,\, (\sum^{T}_{t=1} \nabla_{\theta} \log \pi_{\theta} (a_t \vert s_t)) (\sum^{T}_{t^\prime=t} r(s_t^\prime,a_t^\prime) - V^{\pi}_{\psi}(s_t)) $$

하지만 $$ V^{\pi}_{\psi}(s_t) $$를 baseline으로 사용하는것이 여전히 $$ \nabla_{\theta} J(\theta) $$과 같은 expectation이 될 수 있을까요? 저희의 baseline이 constant가 아닌 $$ s_t $$에 대한 어떤 arbitary function인 $$f(s_t)$$라고 정의한 후, $$ \nabla_{\theta} J(\theta) $$에 대한 $$f(s_t)$$의 expectation을 살펴보도록 하겠습니다.

$$ 
\begin{align*}
\nabla_{\theta} E_{\tau \sim \pi_\theta(\tau)} [\log \pi_{\theta}(\tau) f(s_t)] &=  \int \nabla_{\theta} \pi_\theta(\tau) \pi_{\theta}(\tau) f(s_t) d\tau \\
&= f(s_t) \nabla_{\theta} \int \pi_\theta(\tau) d\tau \\[6pt]
&= f(s_t) \nabla_{\theta}(1) \\[6pt]
&= 0
\end{align*}
$$

따라서 위의 증명과 같이 $$ V^{\pi}_{\psi}(s_t) $$를 baseline으로 사용하여도 policy gradient식은 여전히 $$ \nabla_{\theta} J(\theta) $$에 대한 unbiased estimator를 만족하기 때문에 아무런 문제 없이  $$ V^{\pi}_{\psi}(s_t) $$를 사용할 수 있게 되었습니다.

***Monte-carlo estimate*** 

이제 $$ V^{\pi}_{\psi}(s_t) $$를 어떻게 학습시킬지 정하는 일이 남았습니다. 가장 simple한 방법은 아래 그림과 같이 여러번의 simulation을 통해서 monte-carlo방법으로 값을 근사하는 것입니다. 

$$ V^{\pi}_{\psi}(s_t) \approx \frac{1}{N} \sum^{N}_{i=1} \, (\sum^{T}_{t=1} r(s_{i,t},a_{i,t})) $$

<center><img src = "/assets/images/cs285/lecture6/mc_value.png" width = "200"></center><br>

하지만, simulator를 주어진 $$s_t$$로 reset해가며 sampling을 반복하는것은 실제론 불가능하기 때문에 여러번의 simulation을 통해 value function을 estimate할 순 없습니다. 따라서, 우리는 single sample을 통해서 $$ V^{\pi}_{\psi}(s_t) $$을 estimate할 수 밖에 없고 training data는 $$\{(s_{i,t}, \underbrace{\sum^T_{t^{\prime} = t}r(s_{i,t^{\prime}},a_{i,t^{\prime}})}_{y_{i,t}})\}$$와 같이 생성되게 됩니다.

이후 $$ V^{\pi}_{\psi}(s_t) $$는 주어진 training data에 대하여 $$ L(\phi) = \frac{1}{2}\sum_i \vert \vert V^{\pi}_{\psi}(s_t) - y_i  \vert \vert^2 $$를 minimize하는 방식으로 학습이 진행됩니다. 하지만, single sample을 사용해서 $$s_t$$의 값을 estimate하는 것은 부정확한 결과를 낳지 않을까 걱정 역시 따릅니다.

우선 걱정을 뒤로하고 function approximator의 역할에 대해 되돌아보도록 하죠. Function approximator는 복잡한 input에 대해서 output을 정확하게 맞춤과 동시에 유사한 input에 대해선 유사한 output을 출력하는 generalizability를 확보하기 위해 사용됩니다. 따라서, $$ V^{\pi}_{\psi}(s_t) $$를 학습하는데 있어 서로 다른 trajectory들 안에서도 유사한 state를 방문한다면, value function이 여러번의 simulation을 경험하는것과 비슷한 효과를 낼 수 있게 됩니다.

<center><img src = "/assets/images/cs285/lecture6/mc_fa.png" width = "300"></center><br>

***Temporal-difference estimate*** 

하지만, monte-carlo 방식의 estimation방식은 $$ V^{\pi}_{\psi}(s_t) $$를 학습시키는데 있어서 trajectory의 길이가 길면 길어질 수록 target의 variance가 급격하게 커진다는 문제점이 있습니다. 그렇다면, target에도 estimation을 도입해서 variance를 낮추는 것은 어떨까요? 아래와 같이 학습된 $$ V^{\pi}_{\psi}(s_t) $$를 활용하여 target 자체를 다시 근사하는 것입니다. 

$$ V^{\pi}_{\psi}(s_t) \approx r(s_{t},a_{t}) +   V^{\pi}_{\psi}(s_{t+1}) $$

Training data를 $$\{(s_{i,t}, \underbrace{r(s_{i,t},a_{i,t}) +   V^{\pi}_{\psi}(s_{i,t+1})}_{y_{i,t}}\}$$와 같이 생성하고 이전과 동일하게 $$ L(\phi) = \frac{1}{2}\sum_i \vert \vert V^{\pi}_{\psi}(s_t) - y_i  \vert \vert^2 $$를 minimize하는 방향으로 학습을 진행시키는 것입니다. 이런 방법은 reward의 일시적인 오차를 통해서 $$ V^{\pi}_{\psi}(s_t) $$를 추정하기 때문에 temporal-difference estimate 또는 bootstrapped estimate라고 불리게 됩니다.

### Estimating the returns

$$ \nabla_{\theta} J(\theta) \approx \sum^{N}_{i=1} \,\, (\sum^{T}_{t=1} \nabla_{\theta} \log \pi_{\theta} (a_t \vert s_t)) (\sum^{T}_{t^\prime=t} r(s_{t^{\prime}},a_{t^{\prime}}) - V^{\pi}_{\psi}(s_t)) $$

Baseline이 $$V^{\pi}_{\psi}(s_t) $$로 바뀌게 된 새로운 수식에서 어떤 부분이 또 다시 estimation이 가능할까요? 4강에서 살펴봤던 Q-function의 수식을 돌아보도록 하죠.

$$ 
\begin{align*}
Q(s_t,a_t) &=  \sum^T_{t^{\prime}=t}E_{\pi_{\theta}}[r(s_{t^{\prime}},a_{t^{\prime}}) \vert s_t, a_t] \\
& \approx \sum^{T}_{t^\prime=t} r(s_{t^{\prime}},a_{t^{\prime}})\\[6pt]
\end{align*}
$$

따라서, 우리는 $$ \sum^{T}_{t^\prime=t} r(s_{t^{\prime}},a_{t^{\prime}} $$를 $$Q(s_t,a_t)$$를 통해 estimate할 수 있게 되고 수식은 아래와 같이 바뀌게 됩니다.

$$ \nabla_{\theta} J(\theta) \approx \sum^{N}_{i=1} \,\, (\sum^{T}_{t=1} \nabla_{\theta} \log \pi_{\theta} (a_t \vert s_t)) (\sum^{T}_{t^\prime=t} r(s_{t^{\prime}},a_{t^{\prime}}) - V^{\pi}_{\psi}(s_t)) $$


## Discount factor

## Actor-critic algorithm

### Architecture design

### Bias - variance tradeoff

### GAE(Generalized advantage estimation)

## Thoughts

### Online GAE

### Control variates

## Summing up

[andrewng_mvn]: https://www.youtube.com/watch?v=JjB58InuTqM
[karpathy_pong]: http://karpathy.github.io/2016/05/31/rl/
[ce_mle]: https://medium.com/konvergen/cross-entropy-and-maximum-likelihood-estimation-58942b52517a