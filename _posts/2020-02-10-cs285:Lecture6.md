---
title:  "[CS 285] Lecture 6: Actor-Critic Algorithms"
date:   2020-02-10 12:04:36 +0900
tags:
  - CS285
---
이번 포스팅에서는 policy gradient의 variance를 어떻게 줄일 수 있는지 살펴보고 **actor-critic** 알고리즘에 대해 소개하는 시간을 갖도록 하겠습니다. CS 285 강의를 기본으로 하되 흐름에 맞게 내용들을 추가하고 재배치했음을 밝힙니다. 부족한점에 대한 지적과 질문은 자유롭게 댓글로 남겨주세요.

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

## Policy evaluation

우선 casuality와 baseline을 도입한 policy gradient의 식을 다시 살펴보겠습니다.

$$ 
\begin{align*}
\nabla_{\theta} J(\theta) &=  E_{\tau \sim p_{\theta} (\tau)} [ (\sum^{T}_{t=1} \nabla_{\theta} \log \pi_{\theta} (a_t \vert s_t)) (\sum^{T}_{t^\prime=t} r(s_t^\prime,a_t^\prime) - b)] \\
& \approx \sum^{N}_{i=1} \,\, (\sum^{T}_{t=1} \nabla_{\theta} \log \pi_{\theta} (a_t \vert s_t)) (\sum^{T}_{t^\prime=t} r(s_t^\prime,a_t^\prime) - b)
\end{align*}
$$

비록 기존의 policy gradient의 식에 비해서는 많이 발전했지만, 여전히 variance는 너무나 높습니다. 따라서 우리는 위의 수식을 보다 정교하게 estimate할 수 있는 방법을 고민해보아야 합니다. 

### Improving the baseline

지난 시간에 simple한 baseline으로는 단순히 trajectory들의 average reward를 사용한다고 말씀드렸습니다. 그렇다면, casuality를 반영한 average reward를 보다 정교하게 estimate해보는건 어떨까요? Baseline으로 average reward를 estimate하는 $$V^{\pi}_{\psi}(s_t) $$를 통하여 variance를 낮추는 것입니다.

$$ \nabla_{\theta} J(\theta) \approx \sum^{N}_{i=1} \,\, (\sum^{T}_{t=1} \nabla_{\theta} \log \pi_{\theta} (a_t \vert s_t)) (\sum^{T}_{t^\prime=t} r(s_t^\prime,a_t^\prime) - V^{\pi}_{\psi}(s_t)) $$

하지만 $$ V^{\pi}_{\psi}(s_t) $$를 baseline으로 사용하는것이 여전히 $$ \nabla_{\theta} J(\theta) $$과 같은 expectation이 될 수 있을까요? 저희의 baseline이 constant가 아닌 $$ s_t $$에 대한 어떤 arbitary function인 $$f(s_t)$$라고 정의한 후, $$ \nabla_{\theta} J(\theta) $$에 대한 $$f(s_t)$$의 expectation을 살펴보도록 하겠습니다.

$$ 
\begin{align*}
\nabla_{\theta} E_{\tau \sim \pi_\theta(\tau)} [\log \pi_{\theta}(\tau) f(s_t)] &=  \int \pi_{\theta}(\tau) \nabla_{\theta} \log \pi_\theta(\tau) f(s_t) d\tau \\
&= f(s_t) \nabla_{\theta} \int \pi_\theta(\tau) d\tau \\[6pt]
&= f(s_t) \nabla_{\theta}(1) \\[6pt]
&= 0
\end{align*}
$$

따라서 위의 증명과 같이 $$ V^{\pi}_{\psi}(s_t) $$를 baseline으로 사용하여도 policy gradient식은 여전히 $$ \nabla_{\theta} J(\theta) $$에 대한 unbiased estimator를 만족하기 때문에 아무런 문제 없이  $$ V^{\pi}_{\psi}(s_t) $$를 사용할 수 있게 되었습니다.

***Monte-carlo estimate*** 

이제 $$ V^{\pi}_{\psi}(s_t) $$를 어떻게 학습시킬지 정하는 일이 남았습니다. 가장 simple한 방법은 아래 그림과 같이 여러번의 simulation을 통해서 monte-carlo방법으로 값을 근사하는 것입니다. 

$$ V^{\pi}_{\psi}(s_t) \approx \frac{1}{N} \sum^{N}_{i=1} \, (\sum^{T}_{t=1} r(s_{i,t},a_{i,t})) $$

<center><img src = "/assets/images/cs285/lecture6/mc_value.png" width = "200"></center><br>

하지만, simulator를 주어진 $$s_t$$로 reset해가며 sampling을 반복하는것은 실제론 불가능하기 때문에 여러번의 simulation을 통해 value function을 estimate할 순 없습니다. 따라서, 우리는 single sample을 통해서 $$ V^{\pi}_{\psi}(s_t) $$을 estimate할 수 밖에 없고 training data는 $$\{(s_{i,t}, \underbrace{\sum^T_{t^{\prime} = t}r(s_{i,t^{\prime}},a_{i,t^{\prime}})}_{y_{i,t}})\}$$와 같이 생성되게 됩니다.

이후 $$ V^{\pi}_{\psi}(s_t) $$는 주어진 training data에 대하여 $$ L(\phi) = \frac{1}{2}\sum_i \vert \vert V^{\pi}_{\psi}(s_t) - y_i  \vert \vert^2 $$를 minimize하는 방식으로 학습이 진행됩니다. 하지만, single sample을 사용해서 $$s_t$$의 값을 estimate하는 것은 부정확한 결과를 낳지 않을까?하는 걱정 역시 따릅니다.

우선 걱정을 뒤로하고 function approximator의 역할에 대해 되돌아보도록 하죠. Function approximator는 복잡한 input에 대해서 output을 정확하게 맞춤과 동시에 유사한 input에 대해선 유사한 output을 출력하는 generalizability를 확보하기 위해 사용됩니다. 따라서, $$ V^{\pi}_{\psi}(s_t) $$를 학습하는데 있어 서로 다른 trajectory들 안에서도 유사한 state를 방문한다면, value function이 여러번의 simulation을 경험하는것과 비슷한 효과를 낼 수 있게 됩니다.

<center><img src = "/assets/images/cs285/lecture6/mc_fa.png" width = "300"></center><br>

***Temporal-difference estimate*** 

하지만, monte-carlo 방식의 estimation방식은 $$ V^{\pi}_{\psi}(s_t) $$를 학습시키는데 있어서 trajectory의 길이가 길면 길어질 수록 target의 variance가 급격하게 커진다는 문제점이 있습니다. 그렇다면, target에도 estimation을 도입해서 variance를 낮추는 것은 어떨까요? 아래와 같이 학습된 $$ V^{\pi}_{\psi}(s_t) $$를 활용하여 target 자체를 다시 근사하는 것입니다. 

$$ V^{\pi}_{\psi}(s_t) \approx r(s_{t},a_{t}) +   V^{\pi}_{\psi}(s_{t+1}) $$

Training data를 $$\{(s_{i,t}, \underbrace{r(s_{i,t},a_{i,t}) +   V^{\pi}_{\psi}(s_{i,t+1})}_{y_{i,t}}\}$$와 같이 생성하고 이전과 동일하게 $$ L(\phi) = \frac{1}{2}\sum_i \vert \vert V^{\pi}_{\psi}(s_t) - y_i  \vert \vert^2 $$를 minimize하는 방향으로 학습을 진행시키는 것입니다. 이런 방법은 reward의 일시적인 오차를 통해서 $$ V^{\pi}_{\psi}(s_t) $$를 추정하기 때문에 temporal-difference estimate 또는 bootstrapped estimate라고 불리게 됩니다.

### Estimating the returns

$$ \nabla_{\theta} J(\theta) \approx \sum^{N}_{i=1} \,\, (\sum^{T}_{t=1} \nabla_{\theta} \log \pi_{\theta} (a_t \vert s_t)) (\sum^{T}_{t^\prime=t} r(s_{t^{\prime}},a_{t^{\prime}}) - V^{\pi}_{\psi}(s_t)) $$

Baseline이 $$V^{\pi}_{\psi}(s_t) $$로 바뀌게 된 새로운 수식에서 어떤 부분이 또 다시 estimation이 가능할까요? 4강에서 살펴봤던 Q-function의 수식을 돌아보도록 하죠.

$$ 
\begin{align*}
Q(s_t,a_t) &=  \sum^T_{t^{\prime}=t}E_{\pi_{\theta}}[r(s_{t^{\prime}},a_{t^{\prime}}) \vert s_t, a_t] \\
& \approx \sum^{T}_{t^\prime=t} r(s_{t^{\prime}},a_{t^{\prime}})\\[6pt]
\end{align*}
$$

따라서, 우리는 $$ \sum^{T}_{t^\prime=t} r(s_{t^{\prime}},a_{t^{\prime}}) $$를 $$Q(s_t,a_t)$$를 통해 estimate할 수 있게 되고 수식은 아래와 같이 바뀌게 됩니다.

$$ \nabla_{\theta} J(\theta) \approx \sum^{N}_{i=1} \,\, (\sum^{T}_{t=1} \nabla_{\theta} \log \pi_{\theta} (a_t \vert s_t)) (Q(s_t,a_t) - V^{\pi}_{\psi}(s_t)) $$

그렇다면 $$Q(s_t,a_t)$$값 또한 $$V(s_t)$$와 같이 simulation을 통해서 값을 학습해야 할까요?

$$ 
\begin{align*}
Q(s_t,a_t) &=  \sum^T_{t^{\prime}=t}E_{\pi_{\theta}}[r(s_{t^{\prime}},a_{t^{\prime}}) \vert s_t, a_t] \\
&= r(s_t, a_t) + E_{s_{t+1} \sim p(s_{t+1} \vert s_t, a_t)}[V^{\pi}(s_{t+1})] \\[6pt]
&\approx r(s_t, a_t) + V^{\pi}(s_{t+1})
\end{align*}
$$

위의 수식과 같이 $$Q(s_t,a_t)$$는 $$V(s_t)$$를 활용해서 근사를 할 수가 있기 때문에, baseline으로 학습한 $$ V^{\pi}_{\psi} $$를 활용할 수가 있게 됩니다. 따라서, return까지 estimation을 수행하여 variance를 낮춘 최종적인 수식은 아래와 같습니다.

$$ \nabla_{\theta} J(\theta) \approx \sum^{N}_{i=1} \,\, (\sum^{T}_{t=1} \nabla_{\theta} \log \pi_{\theta} (a_t \vert s_t)) (r(s_t, a_t) + V^{\pi}_{\psi}(s_{t+1}) - V^{\pi}_{\psi}(s_t)) $$

## Discount factor

그렇지만 $$  V^{\pi}_{\psi} $$를 학습시키는데 있어서 아직 난관이 있습니다. $$  V^{\pi}_{\psi}(s_t) $$의 target은 $$ y_{i,t} \approx r(s_{i,t},a_{i,t}) + V^{\pi}_{\psi}(s_{i,t+1}) $$이 됩니다. 따라서 아래의 continuous task와 같이 episode의 길이가 무한대가 된다면,  $$  V^{\pi}_{\psi}(s_t) $$또한 무한대로 발산하게 됩니다.

<center><img src = "/assets/images/cs285/lecture6/continuous.png" width = "500"></center><br>

이를 해결하는 단순한 방법은 discount factor를 도입하여 cumulative rewards를 decaying 시키는 것입니다. Discount factor를 적용한 policy gradient의 수식들은 아래와 같이 바뀝니다.

$$ y_{i,t} \approx r(s_{i,t},a_{i,t}) + \gamma V^{\pi}_{\psi}(s_{i,t+1}), \,\,\,\,\,\,\, \gamma \in [0,1] $$

$$ \nabla_{\theta} J(\theta) \approx \sum^{N}_{i=1} \,\, (\sum^{T}_{t=1} \nabla_{\theta} \log \pi_{\theta} (a_t \vert s_t)) (r(s_t, a_t) + \gamma V^{\pi}_{\psi}(s_{t+1}) - V^{\pi}_{\psi}(s_t)) $$

그렇다면, 기존의 vanilla policy gradient수식에 discount factor를 사용한다면 둘 중에 어떤 수식을 사용해야 할까요?

$$
\begin{align*}
option1 &: \nabla_{\theta} J(\theta) \approx \sum^{N}_{i=1} \,\, (\sum^{T}_{t=1} \nabla_{\theta} \log \pi_{\theta} (a_t \vert s_t)) (\sum^{T}_{t^\prime=t} \gamma^{t^{\prime} - t} r(s_t^\prime,a_t^\prime)) \\
option2 &: \nabla_{\theta} J(\theta) \approx \sum^{N}_{i=1} \,\, (\sum^{T}_{t=1} \gamma^{t - 1} \nabla_{\theta} \log \pi_{\theta} (a_t \vert s_t)) (\sum^{T}_{t^\prime=t} \gamma^{t^{\prime} - t} r(s_t^\prime,a_t^\prime)) \\[6pt]
\end{align*}
$$


곰곰히 식을 생각해보면 option1의 $$ \sum^{T}_{t^\prime=t} \gamma^{t^{\prime} - t} r(s_t^\prime,a_t^\prime) $$이 $$r(s_t, a_t) + \gamma V^{\pi}_{\psi}(s_{t+1}) - V^{\pi}_{\psi}(s_t) $$과 같은 의미의 수식으로 보입니다. 따라서, 여기서의 $$\gamma$$의 역할은 미래에 대한 불확실성을 고려해서 미래의 reward를 줄이는 역할을 수행하는 것입니다. 그렇다면 option2가 의미하는 수식은 무엇일까요? Option2는 $$t=1$$부터 discount를 적용해나가기 때문에, 아래와 같이 매 timestep마다 $$\gamma$$의 확률로 game이 종료되는 형태로 변형된 mdp를 해결하는 것을 의미하게 됩니다.

<center><img src = "/assets/images/cs285/lecture6/death.png" width = "280"></center><br>

또한 discount factor를 variance의 관점에서 생각해본다면 $$V^{\pi}_{\psi}(s_t)$$의 값이 시간에 지남에 따라 빠르게 줄어들기 때문에, $$\gamma $$값이 작을 수록 variance가 줄어드는 효과를 가져오게 됩니다.


## Actor-Critic algorithm

이제 본격적으로 actor-critic 알고리즘에 대하여 알아보도록 하겠습니다.

<center><img src = "/assets/images/cs285/lecture6/actor_critic.png" width = "420"></center><br>

혹시 눈치채셨나요? 오늘 저희가 변형한 policy gradient의 수식이 바로 actor-critic 알고리즘입니다. **Actor-critic알고리즘은 actor $$(\pi_{\theta})$$가 주어진 state에서 행동을 결정하고, critic$$ (V^{\pi}_{\psi}) $$은 이러한 actor의 행동을 평가하는 알고리즘을 의미하는 것입니다.** 그렇다면 actor-critic 알고리즘은 기존의 policy gradient에 비해 variance를 낮췄으니 더 좋은 알고리즘이라고 말할 수 있을까요? 

저희가 policy gradient 알고리즘의 baseline으로 $$ V^{\pi}_{\psi} $$를 사용하더라도 여전히 $$ \nabla_{\theta} J(\theta) $$에 대한 unbiased estimator를 만족한다고 말씀 드렸습니다. 하지만, return을 estimate할 때 $$ V^{\pi}_{\psi} $$를 사용한다면 $$ V^{\pi}_{\psi} $$에 대하여 정확하지 않은 값을 estimation에 사용하기 때문에 자연스레 bias가 발생하게 됩니다. 즉, actor-critic 알고리즘은 **bias-variance tradeoff**를 통해서 variance를 줄이고 bias를 발생시키게 되는 알고리즘입니다.

Actor-critic 알고리즘에서 baseline을 사용하는것 역시 저희의 option이 되는데 baseline으로 $$V^{\pi}_{\psi}$$를 사용했을 때, 

$$r(s_t, a_t) + \gamma V^{\pi}_{\psi}(s_{t+1}) - V^{\pi}_{\psi}(s_t) \approx Q^{\pi}(s_t,a_t)- V^{\pi}_{\psi}(s_t) = A^{\pi}(s_t,a_t) $$

가 되기 때문에, advantage function을 maximize시킨다고 하여 advantageous actor-crtic으로 불리기도 합니다.

### GAE(Generalized advantage estimation)

위의 내용을 정리해보자면 다음과 같습니다.

- **policy gradient:** no bias, high variance
- **actor-critic:** high bias, low variance

그렇다면, 저희가 bias-variance tradeoff를 직접 조절할 수는 없을까요? Trajectory를 끝까지 다 보는게 variance가 높고 다음 timestep의 state만 보는것이 bias가 높다면 n-step뒤까지만 trajectory를 보는 것은 어떨까요?

$$ A^{\pi}_{n}(s_t,a_t) = \sum^{t+n}_{t^\prime=t} \gamma^{t^{\prime} - t} r(s_t^{\prime},a_t^{\prime}) - V^{\pi}_{\psi}(s_t) + \gamma^n V^{\pi}_{\psi}(s_{t+n}) $$

이러한 형태의 return계산을 우리는 **n-step returns**라고 말합니다. 그런데 여기서 의문이 하나더 생깁니다. 한 개의 n-step return이 아니라 여러개의 n-step return을 동시에 사용해서 값들을 average시키는 것입니다. 이렇게 advantage를 계산하는 방법을 **GAE**(Generalized advantageous estimation)이라고 부릅니다.

$$ A^{\pi}_{GAE}(s_t,a_t) = \sum^{\infty}_{n=1} w_n A^{\pi}_{n}(s_t,a_t) $$

이젠 각각의 n-step return에 대해서 weight를 어떻게 부여할지 정하는것 만이 남았습니다. 이전의 discount factor와 같이 n이 작은 값에 ㄴ먼저 높은 가중치를 준 후 exponential하게 decaying해 나가는 것입니다.

<center><img src = "/assets/images/cs285/lecture6/gae.png" width = "750"></center><br>

유도된 두번째 식에서 보듯이, $$\lambda$$는 각 timestep의 temporal difference에 대해서 discount factor, $$\gamma$$와 같은 효과를 일으키고 있습니다. 따라서, $$\lambda$$값 역시 작을 수록 return의 variance를 줄어들게 할 수 있습니다. 

구현에 있어서도 첫번째 식이 아니라 두번째 식을 활용한다면 아래와 같이 GAE를 쉽고 효율적으로 구현할 수 있습니다.

{% highlight python %}
"""
trajectory: (s1,a1,r1,s2,a2,r2, ... ,sT)
rewards: [r1, r2, ..., r(T-1)]
v_preds: [V(s1),V(s2), ..., V(sT)]
"""
# Temporal difference
delta = rewards + _gamma * value_preds[1:] - value_preds[:-1]
G = 0
targets = []
for delta_t in delta[::-1]:
    G = delta_t + _gamma *_lambda* G
    targets.append(G)
return targets[::-1]
{% endhighlight %}

### Architecture design

Actor-critic 알고리즘을 구현하는데 있어서 몇가지 architecture의 선택지들이 존재합니다. 첫번째는 actor network와 policy network가 feature를 공유할 것인지에 대한 내용입니다. 

<center><img src = "/assets/images/cs285/lecture6/sep_net.png" width = "280" style = "margin-right:12px"><img src = "/assets/images/cs285/lecture6/share_net.png" width = "280" style = "margin-left:12px"></center><br>

왼쪽과 같이 shared network design을 선택하게 되면 network간의 feature공유가 없기 때문에 training과정이 stable하지만, deep한 network architecture를 사용한다면 feature를 학습하는데 시간이 오래걸리기 때문에 적합하지 않은 구조일 수 있습니다. 실제로 대부분의 actor-critic 논문들은 오른쪽과 같이 shared network형태로 구현되어 있기도 합니다.

두번째 architecture 선택지는 network의 파라미터를 synchronous하게 update할지 asynchronous하게 update할지 선택하는 것입니다. $$ V^{\pi}_{\psi} $$를 잘 학습하기 위해서는 single sample이 아닌 batch로 학습하는 것이 적합하기 때문에 simulation 절차를 parallelize하는게 큰 도움이 됩니다.

<center><img src = "/assets/images/cs285/lecture6/a2c_a3c.png" width = "600"></center><br>

오른쪽과 같이 update를 진행하는 알고리즘을 asynchronous advantageous actor-critic 줄여서 **A3C**라고 부르는데, 왼쪽의 synchronous한 알고리즘은 이와 구별짓기 위하여 **A2C**라고 불리게 됩니다. Deepmind가 2016년 [Asynchronous Methods for Deep Reinforcement Learning
][a3c]를 발표한 이후, asynchronous한 update의 noise가 일종의 regularizer로서 동작하면서 성능을 오히려 향상시켜줄 수 있을지 많은 의문을 가졌습니다. 하지만 OpenAI에 따르면, 그들의 실험들에선 이런 noise가 성능을 향상시켜준다는 empirical한 증거는 없다고 말합니다.

<center><img src = "/assets/images/cs285/lecture6/a2c_openai.png" width = "600"></center>
<center>From 'OpenAI Baselines: ACKTR & A2C'</center><div style = "margin-bottom:12px"></div>

일반적으로 simulation 과정에 있어서도 A2C는 batch형태로 sampling이 진행되기 때문에 gpu가 확보되어 있거나 policy가 크다면 A2C를, 아니라면 A3C를 사용하는 것이 cost-effective합니다.

### Tricks

사실 직접 A2C를 구현하는데 있어서 가장 힘들었던 점은 실제 training 과정이 상당히 불안정하고, overfitting이 쉽게 일어난다는 것이였습니다. 강화학습 알고리즘의 benchmark로 종종 사용되는 **OpenAI: Baseline**의 A2C 코드를 살펴보니 이런 문제점들을 개선하기 위해 2가지 정도의 trick이 사용되고 있었습니다. 첫번째는 entropy loss입니다. Policy distribution의 entropy maximization을 loss로 추가하여 action의 다양성을 끊임없이 유지해주기 위해 노력합니다. 두번째는 initialization trick입니다. Actor network의 output의 variance가 커지게 되면 학습이 채 되기전에 policy가 쉽게 특정 action으로 수렴할 수 있기 때문에, actor network의 final layer에서만 weight를 xavier intialization이 아닌 output의 분산이 0.01에 가깝도록 하는 독특한 initialization을 사용합니다.

<center><img src = "/assets/images/cs285/lecture6/entropy.png" width = "550"></center><div style = "margin-bottom:6px"></div>
<center>Training agent for first-person shooter game with actor crtic curriculum learning. Wu et al. </center><div style = "margin-bottom:18px"></div>

<center><img src = "/assets/images/cs285/lecture6/init.png" width = "500"></center><div style = "margin-bottom:12px"></div>
<center>From OpenAI: Baseline, A2C implementation</center><div style = "margin-bottom:12px"></div>

## Summing up

이번 강의에서 다룬 내용을 정리하자면 아래와 같습니다.

1. Reducing the variance of policy gradient
* Improving the baseline with a value function
* Discount factor
* Estimating the returns with critic
* Generalized advantageous estimation

2. Implementing actor-critic algorithm
* shared network vs seperate network
* synchronous vs asynchronous
* entropy loss, weight initialization

<br>
우리는 4강과 5강에서 policy gradient 알고리즘을 살펴보았고, 6강에선 critic을 통해 actor를 개선하는 방법을 살펴보았습니다. 다음 강의에선 반대로 actor가 없이 critic만으로 학습을 진행하는 방법을 살펴보도록 하겠습니다. 감사합니다.


[a3c]: https://arxiv.org/abs/1602.01783
[andrewng_mvn]: https://www.youtube.com/watch?v=JjB58InuTqM
[karpathy_pong]: http://karpathy.github.io/2016/05/31/rl/
[ce_mle]: https://medium.com/konvergen/cross-entropy-and-maximum-likelihood-estimation-58942b52517a