---
title:  "[CS 285] Lecture 1: Introduction"
date:   2020-02-03 12:04:36 +0900
tags:
  - CS285
---

안녕하세요! 카이스트 인공지능 대학원, 데이터 시각화 연구실의 석사과정에 재학중인 이호준이라고 합니다. 강화학습을 공부하는데 있어 정말 좋은 참고 자료들이 많지만, 이해가 쉽게 되지 않을때면 한글로된 자료들이 부족한것이 항상 아쉬웠습니다. 이번에 같은 연구실의 이병근 학우와 Sergey Levine교수님의 **CS 285: Deep Reinforcement Learning, Decision Making, and Control**을 공부하며 배운 내용들을 한글로 정리하고자 합니다. 많이 부족한 정리 자료이지만, 이 강의를 공부하시는 분들에게 조금이라도 도움이 되기를 소망합니다. 노트는 CS 285 강의를 기본으로 하되 흐름에 맞게 내용들을 추가하고 재배치했음을 밝힙니다. 부족한점에 대한 지적과 질문은 자유롭게 댓글로 남겨주세요.

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

이번 포스팅에서는 앞으로의 강의에서 어떤 내용들을 다룰지 소개하는 시간을 갖습니다. 첫번째 강의인만큼 이론적인 설명은 크게 없이 high-level에서의 직관적 설명을 위주로 강화학습을 다룹니다.

## What is reinforcement learning?

<center><img src = "/assets/images/cs285/lecture1/bim.png" width = "650"></center><br>

이 강의의 주제인 **강화학습**이 무엇인지 살펴보기에 앞서, 우리가 강화학습을 공부하는 보다 근원적인 목적인 인공지능을 어떻게 하면 만들 수 있을지 생각해 봅니다. 이전에 머신러닝을 연구하시는 페드로 도밍게스 교수님의 책, 마스터 알고리즘을 재밌게 읽은 기억이 있습니다. 교수님은 머신러닝 알고리즘들을 그 접근법에 따라서 여러가지 부족들로 나누셨습니다. 인간의 두뇌가 어떻게 생각하는지를 모방하여 인공지능을 만드려는 **연결주의**, 인간의 지식을 귀납적으로 기호화하고, 기호의 관계를 통해 정답을 유추하는 **기호주의**, 다윈의 진화론을 바탕으로 자연선택을 컴퓨터로 모의 실험하는 **진화주의**등 다양한 접근 방법들이 소개되어있습니다. 교수님은 위와 같은 다양한 접근 방법들을 융합하고 발전시켜 나감으로서 진정한 인공지능에 다가갈 수 있다고 말씀하십니다. **지도학습**은 이런 다양한 부족들의 알고리즘을 학습시키는데 보편적으로 사용하는 방식입니다. 이미지, 텍스트, 상황등의 다양햔 데이터를 입력으로 부여하고, 입력에 대응하는 명시적인 타겟의 패턴을 통해 알고리즘을 학습시켜 나갑니다.

<center><img src = "/assets/images/cs285/lecture1/adapt.png" width = "700"></center><br>

하지만, 이러한 지도학습 방법론은 우리가 실제로 행동하며 학습하는 방법과는 큰 차이가 있습니다. 우리들은 살아가며 다양한 의사결정들을 수행합니다. 하지만, 과연 어떠한 의사결정이 좋은 결정이였는지 평가하는것은 정말로 어렵습니다. 단기적으로는 좋아보였던 선택이 장기적으로는 좋지 못했던 선택이 될 수도 있고, 반대로 단기적으로는 나빠보이던 선택지가 장기적으로는 정말 좋은 선택일 수도 있습니다. 우리는 상당히 오랜 시간이 지나고 나서야만, 의사결정 이후에 마주했던 다양한 상황들을 되돌아보며 선택을 평가할 수 있을 뿐입니다. 

<center><img src = "/assets/images/cs285/lecture1/mdp.png" width = "300"></center><br>

**강화학습은 지도학습으로는 쉽게 모델링할 수 없는 우리의 연속된 의사결정들을 모델링하고 평가하기 위한 알고리즘입니다.** 지도학습과 방법론과 같이 인공지능 모델에게 언제나 정답을 알려주는 직접적인 선생님이 있는것이 아니라, 스스로의 경험을 통해서 어떠한 선택이 올바른 선택이었는지 알아내는 것입니다. 저희의 인공지능 agent는 주어진 environment에서 자신이 관찰한 observation에 따라 action을 취하고, action에 따른 보상 reward를 받게됩니다. 이후, 시간이 지난 뒤에 보상의 합을 통해서 어떠한 action이 좋은 action이였는지 알아가는 방식으로 학습을 진행하는 것입니다. 

사실 모델을 학습시키는데 있어서 강화학습 방법론과 지도학습 방법론을 명확히 구분짓는것은 상당히 모호하기도 합니다. 강화학습 알고리즘 역시 주어진 입력에 대해서 최적의 행동을 분류하는 형태로 학습이 진행되기 때문입니다.  하지만, 앞서 설명드린것과 같이 **강화학습은 행동의 순서와 시간을 포괄하여 학습하기 때문에 현재의 reward을 최대화하는 행동을 선택하는 것이 아니라, long-term cumulative rewards을 최대화하는 행동을 선택하는 방향으로 학습을 진행하게 됩니다.** 

## Why should we learn deep reinforcement learning?

사실 강화학습이라는 연구 분야는 상당히 오랜시간 동안 각광받지 못해왔던 연구 분야입니다. 그렇다면 도대체 어떠한 요인들로 인해 강화학습이 다시 주목받게 되었을까요? Levine교수님은 3가지 이유를 통해 강화학습이 주목받게 된 이유를 설명해주십니다.

**3 reasons for the success of reinforcement learning**
- Advances in deep learning
- Advances in reinforcement learning
- Advances in computational capability

위의 3가지 이유중에서도 우리는 특히 deep learning의 발전에 주목할 필요가 있습니다.

### End-to-End learning

Deep learning이 기존 머신러닝 방법론들과 가장 크게 차별화되는 점은 학습이 **end-to-end** 방식으로 진행된다는 것에 있습니다.

<center><img src = "/assets/images/cs285/lecture1/deep_rl.png" width = "700"></center><br>

기존 머신러닝은 각 분야의 domain expert가 feature engineering을 진행한 후 classification을 진행했던 한편, 딥러닝은 image, text와 같은 raw한 data를 집어넣어면 classification을 위한 feature까지 한번에 학습을 진행하게 됩니다. 이와 마찬가지로 강화학습 알고리즘이 발전된 딥러닝 알고리즘과 결합하게되어 훨씬 더 복잡한 문제들을 end-to-end로 학습시킬수 있게 된 것이지요. 따라서, 오늘날의 deep reinforcement learning은 우리가 체스나 바둑에 대한 어떠한 전문가적 지식 없이도 이러한 게임들을 정복할 수 있는 힘을 가지게 되었습니다.

조금 더 자세하게 살펴보면 기존에는 로봇을 제어하기 위해선 아래와 같이 복잡한 pipeline이 필요했습니다.
<center><img src = "/assets/images/cs285/lecture1/robotic_pipeline.png" width = "700"></center><br>

하지만, deep reinforcement learning을 이용한다면 아래와 같이 훨씬 더 단순화된 pipeline을 구축할 수 있게 됩니다.

<center><img src = "/assets/images/cs285/lecture1/robotic_drl_pipeline.png" width = "650"></center><br>

### Applications

오늘날 deep reinforcement learning 알고리즘들은 게임, 로보틱스, 자율주행등 다양한 분야에서 놀라운 성과를 보여주고 있습니다. 현대 인공지능의 대명사로 자리잡은 **알파고(AlphaGo)**역시 deep reinforcement learning의 가장 대표적인 알고리즘중 하나입니다.

<center><img src = "/assets/images/cs285/lecture1/applications.png" width = "650"></center><br>

## What are the challenges?

그럼에도 불구하고, 강화학습 알고리즘들을 현실 세계에 적용시키기 위해서는 여전히 많은 문제점들이 있습니다. 가장 대표적인 문제는 현실세계에서 올바른 reward를 설정하는것이 몹시 어렵다는 것입니다.

<center><img src = "/assets/images/cs285/lecture1/atari.png" width = "250"><img src = "/assets/images/cs285/lecture1/gazel.png" width = "300"></center><br>

강화학습을 통하여 위와 같은 벽돌깨기 게임을 학습시키는 것은 상당히 직관적인 목표를 가지고 있습니다. 게임의 점수를 최대화하는 것이 목표이기 때문에 게임의 점수를 reward로 설정하고 학습을 진행하면 됩니다. 하지만, 오른쪽 그림과 같이 치타가 영양을 잡는 상황을 생각해봅시다. 처음부터 치타가 영양을 잡는것은 매우 어렵기 때문에 쉽게 할 수 없는 경험입니다. 영양을 잡는것을 보상으로 설정한다면, 보상이 너무 희귀하기 때문에 학습의 난이도가 이전과 달리 몹시 어려워지게 됩니다. 그렇다면 영양을 아쉽게 놓치는 경우 역시 보상으로 설정해 주는것은 어떨까요? '아쉽다'라는 정도를 수학적으로 정의하고 보상으로 설계하는것 역시 쉬운 일은 아닙니다. 또한, 영양을 잡아야하는 본래의 목적과는 전혀 다르게 영양을 계속 아쉽게 놓치는 방향으로 학습이 진행될지도 모릅니다. 

이렇게 현실세계의 문제들을 해결하기 위해서는 보상과 관련된 다양한 문제점들이 뒤따릅니다. 이러한 문제들을 어떻게 효과적으로 대처할 수 있을까요? 우선, 다양한 예시들을 활용해서 학습에 도움을 줄 수 있습니다. 가장 단순한 방법은 다른 agent들의 행동을 그대로 따라하는 **Imitation Learning**을 진행하는 것입니다. 자율주행 자동차를 예로 들면, 상황별로 이전의 운전자가 했던 행동을 그대로 따라하도록 학습을 진행하는 것입니다. 

<center><img src = "/assets/images/cs285/lecture1/imitation.png" width = "650"></center><br>

하지만, 사람들은 다른 사람들의 행동을 살펴보며 행동을 그대로 따라하는것이 아니라 행동뒤에 숨겨진 의미를 파악할 수도 있습니다. 아래의 아기와 같이 말이죠. **Inverse reinforcement learning**은 이러한 아이디어가 녹아있는 강화학습 알고리즘입니다.  기존 agent의 행동이 보상을 최대화 하기 위한 행동이었다고 가정하고 보상 함수를 역으로 추정하여 학습을 진행하는 것입니다.

<center><img src = "/assets/images/cs285/lecture1/inverse_rl.gif" width = "450" style = "margin-bottom:12px"></center>
<center>Warneken & Tomasello et al</center><div style = "margin-bottom:12px"></div>

학습을 보다 효율적으로 진행하기 위해 주어진 환경을 먼저 이해시키는것에 집중할 수도 있습니다. **Model based learning**을 활용해서 다음에 어떤 일이 일어날지 prediction을 해보고 prediction을 바탕으로 가장 좋은 행동을 취하는 것이지요. 아래의 동영상은 이런 model-based방식의 하나의 예제입니다. 또는 오토인코더와 같은 **Unsupervised learning**을 활용하여 초창기 모델의 파라미터로 초기화해주는것 역시 학습의 효율을 크게 높여줄 것입니다.

<center><img src = "/assets/images/cs285/lecture1/prediction.gif" width = "300" style = "margin-right:12px"><img src = "/assets/images/cs285/lecture1/manipulation.gif" width = "300" style = "margin-left:12px"></center>
<div style = "margin-bottom:12px"></div>
<center>Karl Schmeckpeper et al. 2019</center><div style = "margin-bottom:12px"></div>

이전에 학습했던 다른 해결책들을 활용할 수도 있습니다. **Transfer learning**을 통해 이전에 비슷한 task에 대해 학습을 진행할 때 배운 행동들을 전이 할 수도 있고, **Meta learning**을 통해 처음부터 다양한 task에 활용될 수 있는 보편적인 행동을 학습할 수도 있습니다.

따라서, 현실 세계의 다양한 문제들을 풀기 위해선 보다 efficient하고 generalized된 모델을 학습시키는 것이 중요하고 해결해야할 여러가지 난관들이 있습니다. 앞으로의 강의에선 이런 난관들을 해결하기 위한 다양한 해결책들을 좀 더 자세하게 살펴보도록 하겠습니다.

## Summing up

이번 강의에서 다룬 내용을 정리하자면 아래와 같습니다.

1. What is reinforcement learning
2. Why do we need to learn deep reinforcement learning now?
3. Challenges & solutions for the deep reinforcement learning
- Learning from demonstrations : Imitation learning, Inverse reinforcement learning
- Learning from other tasks : Transfer learning, Meta-learning
- Learning from observing the world : Model-based learning, Unsupervised learning


<br>
이번 강의는 첫번째 강의였던 만큼 이론적인 내용보다는 앞으로 배울 내용들을 소개하는데 초점이 맞춰져있던것 같습니다. 다음 시간에는 강화학습이 해결하고자 하는 sequential decision problem을 좀 더 formal하게 정의하고, 기존의 지도학습 방법론을 통해 해결하는 방법들을 소개하는 시간을 갖도록 하겠습니다. 감사합니다.