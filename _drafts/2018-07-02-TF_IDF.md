---
title:  "TF-IDF Tutorial"
date:   2018-07-02 14:04:36 +0900
tags:
  - NLP
---

포털 검색은 어느새부터 우리의 일상이 됬습니다. 과거에는 필요한 정보가 있을 때면 도서관에서 관련 책들을 쌓아 놓고 하나하나 찾았던 반면, 지금은 구글과 같은 포털 사이트에서 검색 한번을 통해 원하는 정보를 한번에 찾아낼 수가 있습니다. 컴퓨터는 어떻게 우리의 검색에 맞는 문서를 척척 찾아줄까요? 컴퓨터의 검색을 도와주는 다양한 알고리즘이 있지만, 그중에서 가장 대표적인 알고리즘인 **TF-IDF** 알고리즘을 다뤄보고자 합니다. **TF-IDF**란 저희가 검색하는 **키워드**에 가장 부합하는 **문서**를 찾아주는 알고리즘입니다. 

이번 포스팅에서는 **TF-IDF**의 기본 개념이 되는 **Vector Space Model**과 **TF**, **IDF**개념을 각각 살펴보고 **Python**을 통해서 문서를 주제별로 분류하는 Tutorial을 진행해 보겠습니다. 이 포스팅은 고려대학교 **이상근** 교수님의 강의를 많이 참조하였습니다.

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

## Vector Space Model

<img src = "/assets/images/document.png" width = "100"> <font size ="15">&rarr;</font>
<img src = "/assets/images/vector_space.png" width = "220">

**Vector Space Model** 또는 **Term Vector Model**은 단순하게 텍스트 또는 어떤 문서를 벡터로 표현하는 방법입니다. 우리는 벡터의 각 차원을 문서들에서 사용되는 *unique*한 단어들로 구성합니다. 모든 문서에서 사용되는 *unique*한 단의 개수가 *m*개 일때, *Vector Space Model*에서 어떠한 문서 *d*는 *m-dimensional vector*로 나타내어 지게 됩니다. 

Vector space Model중에 가장 간단한 모델인 **Boolean Model**을 통해 예제와 함께 알아보도록 하죠.

## Boolean Model  

**Boolean Model**은 정말 간단합니다. 각 문서에 대하여 단어 $$ t_{i} $$가 존재하면 1 않는다면 0 값을 부여해주는 것입니다.

각각 한 문장으로만 구성이 된 3개의 문서를 살펴보도록 하죠.

$$ d_{1} $$ : *The Coffee bean serves the best chocolate and coffee.*

$$ d_{2} $$ : *The best serves in tennis*

$$ d_{3} $$ : *Chocolate and coffee are the best partner.*

문서내에 존재하는 unique한 단어들은 11개로서, 각 문서는 *11-dimensional vector*로서 나타낼 수 있게 됩니다.

|    | the | coffee | bean | serves | best | chocolate | and  | in | tennis | are | partner |
|----|:---:|:------:|:----:|:------:|:----:|:---------:|:----:|:--:|:------:|:---:|:-------:|
|$$ d_{1} $$| 1   | 1      | 1    | 1      | 1    | 1         | 1    | 0  | 0      | 0   | 0       |
|$$ d_{2} $$| 1   | 0      | 0    | 1      | 1    | 0         | 0    | 1  | 1      | 0   | 0       |
|$$ d_{3} $$| 1   | 1      | 0    | 0      | 1    | 1         | 1    | 0  | 0      | 1   | 1       |

  
위와 같이 우리는 문서를 *Boolean Model*을 통해 쉽게 벡터로 변환시켰습니다. 그렇다면, 문서를 벡터화 한다면 무엇을 할 수 잇을까요? 2개의 벡터가 있다면 **Cosine Similarity**를 통해 쉽게 벡터들간의 유사도를 측정할 수 있습니다. 즉, 문서간의 유사도를 측정할 수 있게 되는 것이지요.
<br>
### Cosine similarity ###
<font size = "2">
$$ similarity(d_{1}, d_{2}) = \cos(\theta) = \frac{d_{1} \cdot d_{2}}{|d1| |d2|} $$
</font>
<br>
*Cosine similarity*는 두 벡터간의 각도의 코사인 값을 이용해서 측정된 벡터의 유사도를 의미합니다. 두 벡터의 각도가 0°로 완전히 일치할 경우 1의 값을, 90°로 독립적일 경우 0의 값을, 180°로 완전히 반대를 가리킬 때 -1의 값을 갖게 됩니다.

*Cosine similarity*를 바탕으로 세 문서 $$ d_{1}, d_{2}, d_{3} $$의 유사도를 측정해보도록 하죠. 
<font size = "2">
$$ similarity(d_{1}, d_{2}) = \frac{3}{\sqrt{7} \cdot \sqrt{5}} = \frac{3}{\sqrt{35}} $$

$$ similarity(d_{1}, d_{3}) = \frac{5}{\sqrt{7} \cdot \sqrt{7}} = \frac{5}{7} $$

$$ similarity(d_{2}, d_{3}) = \frac{2}{\sqrt{5} \cdot \sqrt{7}} = \frac{2}{\sqrt{35}} $$
</font>
초콜릿과 커피를 공통적으로 이야기 하고 있는 $$ d_{1} $$과 $$ d_{3} $$가 가장 유사도가 높은 것으로 측정되었습니다. 이와같이, *Boolean Model*을 통해서 문서간의 유사도를 쉽게 확인할 수 있엇습니다. 하지만 중요한 문제가 있습니다. $$ d_{1} $$과 $$ d_{3} $$에서 *coffee*라는 단어를 아무리 많이 언급해도 *Cosine similarity*는 변하지 않게 됩니다. $$ d_{1} $$에서 *coffee*는 2번 언급 되었지만, *coffee*와 *chocolate*은 같은 중요도로 판별되고 있습니다. 이런 문제점을 **TF Model**은 훌륭하게 해결합니다.

## TF Model  

**TF Model(Term Frequency Model)**은 **Boolean Model** 정말 조금만 변형시키면 됩니다. 각 문서에 대하여 단어 $$ t_{i} $$의 출현 여부를 판단하는 것이 아니라 단어 $$ t_{i} $$가 출현한 **빈도(Term frequency)**를 측정하는 것입니다.

따라서, 문서 $$ d $$에서 단어 $$ t $$의 총 빈도를 $$ f(t,d) $$라고 할 때 
$$ tf(t,d) = f(t,d) $$가 됩니다.

|    | the | coffee | bean | serves | best | chocolate | and  | in | tennis | are | partner |
|----|:---:|:------:|:----:|:------:|:----:|:---------:|:----:|:--:|:------:|:---:|:-------:|
|$$ d_{1} $$| 2   | 2      | 1    | 1      | 1    | 1         | 1    | 0  | 0      | 0   | 0       |
|$$ d_{2} $$| 1   | 0      | 0    | 1      | 1    | 0         | 0    | 1  | 1      | 0   | 0       |
|$$ d_{3} $$| 1   | 1      | 0    | 0      | 1    | 1         | 1    | 0  | 0      | 1   | 1       |


빈도가 큰 단어들의 영향력이 급격하게 커지는것을 막기 위해서 로그 스케일 빈도를 사용할 수 도 있습니다. 이때는 
$$ tf(t,d) = log(f(t,d) +1) $$이 됩니다.

하지만, **TF Model**또한 큰 문제점이 뒤따릅니다. 네이버에 올라오는 기사들을 한번 살펴보도록 하죠. 각 기사는 수많은 문장들로 구성 되어 있으며, 기사는 풍부한 어휘로 구성되어 있습니다. 같은 경제면의 기사이더라도 사용하는 어휘는 상당히 다를 수 밖에 없습니다. 이런 기사들의 **Cosine Similarity**를 계산한다면, 유사도에서 주도적인 역할을 하는 *feature*들은 *the*, *a*, *is*와 같은 특별한 의미 없는 단어들이 됩니다. 우리는 *economy*, *finance*와 같은 단어들이 기사에서 등장한다면 스포츠 기사보다는 경제 기사일 확률이 높다는 것을 알고 있지만, **TF Model**은 *economy*라는 단어와 *the*라는 단어의 중요도를 같게 평가합니다. 이러한 문제를 해결하기 위해서 **TF-IDF Model**이 제안됩니다.

## TF-IDF Model 

**IDF(Inverse Document Frequency)**를 직역하자면 문서 빈도의 역수값입니다. 우리는 단어 $$ t_{i} $$의 중요도를 평가하기 위해서 $$ t_{i} $$가 얼마나 다양한 문서들에서 출현하는지를 측정합니다. 측정된 값 $$ df(t,D) $$가 클수록 흔하게 볼 수 있는 단어일 것이고, 작을 수록 희귀한 단어일 것입니다. 따라서, **IDF**값은 전체 문서의 개수에서 $$ df(t,D) $$를 나누어 준 뒤, 로그를 취해서 쉽게 구할 수 있습니다.

$$ idf(t,D) = log(\frac{N}{df(t,D)}) $$

이제 우리는 문서별로 단어의 출현빈도와 단어의 희귀도를 통해서 문서를 벡터로 변환할 수 있습니다.


$$ tfidf(t,d,D) = tf(t,d) \times idf(t,D) $$

<br>
## Python Implementation

저희는 지금까지 **TF-IDF Model**의 개념을 살펴보고 문서를 효율적으로 벡터로 변환시킬 수 있게 되었습니다. 지금 부터는 **TF-IDF Model**을 통해 실제 60개의 기사들을 활용하여 기사의 주제 분류기를 만들어 보겠습니다. 기사들은 연예 기사 20개, 스포츠 기사 20개, 정치 기사 20개로 이루어져있습니다. 해당 코드는 **Python**으로 구현되었으며  [tf-idf tutorial][tf_github]에서 확인하실 수 있습니다.

### Data Import ###
{% highlight ruby %}
import os
import numpy as np

#현재 위치를 기반으로 문서들의 이름과 절대 경로를 찾습니다.
def get_relative_path(path): 
    filenames = []
    filepaths = []
    for root, dirs, files in os.walk(path):    
        for file in files:
            filepath = os.path.join(root,file)
            filepaths.append(filepath)
            filenames.append(file)
    return filenames,filepaths
{% endhighlight %}

### Data Preprocessing ###

위의 *chocolate & coffe*문장의 예제에서는 *the, a*와 같은 단어들도 벡터화를 시켰었지만, 해당 단어들은 크게 의미를 가지고 있지 않은 경우가 대부분입니다. 위와 같은 단어들을 **stopwords**라고 부르는데 문서를 벡터화 하기 이전에 *preprocessing*을 통해 제거해 주겠습니다. 또한, *economy, Economy, EcoNomy*와 같은 단어들을 통일해 주기 위해서 문서의 단어들을 모두 소문자로 바꿔 줄것입니다. 이러한 작업들을 효과적으로 처리하기 위해 훌륭한 자연어처리 패키지인 **NLTK 라이브러리**를 사용할 것입니다.
{% highlight ruby %}
import nltk
from nltk.corpus import stopwords

def read_docs(path):
    filenames,filepaths = get_relative_path(path)
    docs = []
    doc_names = []
    stopwrds = stopwords.words('english')   #영어의 stopword ex)the를 찾는다
    for filepath,filename in zip(filepaths,filenames): #문서를 하나씩 읽는다.
        f = open(filepath, 'r', encoding='utf-8')
        doc = ''
        temp =  f.readlines()
        for line in temp: 
            doc += line.lower() #단어를 소문자로 변환
        doc = nltk.word_tokenize(doc)
        doc = [token for token in doc if token not in stopwrds]   #stopwords에 없는 단어만 doc에 저장
        docs.append(doc)
        doc_names.append(filename)
    return doc_names, docs

doc_names, docs = read_docs('data') 
{% endhighlight %}

<br>
이제 잘 정제된 문서들을 통해서 **TF-IDF matrix**를 생성할 차례입니다. $$ tf $$와 $$ idf $$값을 쉽게 구할 수 있도록 **index**와 **inverted_index**를 우선 생성해줍니다.

{% highlight ruby %}
def index_doc(docs,doc_names):
    index = {}
    inverse_index = {}
    #문서별로 단어의 개수를 count해서 index생성
    for doc,doc_name in zip(docs,doc_names):
        word_count={} 
        for word in doc:                      
            if word in word_count.keys():
                word_count[word]+=1
            else:
                word_count[word]=1
        index[doc_name] = word_count
    
    #단어별로 출현하는 문서를 찾는 inverse_index생성
    for doc in index.keys():
        doc_index = index[doc]
        for word in doc_index.keys():
            if word in inverse_index.keys():    
                inverse_index[word].append(doc)
            else:                   
                inverse_index[word] = [doc]     
    return index, inverse_index
{% endhighlight %}

{% highlight ruby %}
index, inverted_index = index_doc(docs,doc_names)
print(index)
#->{'doc1':{'chocolate':1, 'coffee':2, ...},'doc2':{...}...}
print(inverse_index)
#->{'chocolate':[doc1,doc3], 'serves':[doc1,doc2]...}
{% endhighlight %}
<br>
다음, 각각의 document와 word가 TF-IDF matrix의 어느 행과 열을 가르키는지 알기 위한 **dictionary**를 만들어 줍니다.
{% highlight ruby %}
def build_dictionary(index):
    dictionary = {}
    for word in index.keys():
        dictionary[word]=len(dictionary)
    return dictionary

word_dictionary = build_dictionary(inverted_index) 
doc_dictionary = build_dictionary(index)
print(word_dictionary)
#->{'former':0,'world':1,'golf':2 ....}
print(doc_dictionary)
#->{'doc13':0,'doc4':1,'doc3':2 ...}
{% endhighlight %}
<br>

### Build TF-IDF matrix ###

이제 준비가 끝났습니다. 문서의 총 개수는 $$ D $$ 이고 모든 단어의 종류의 개수는 $$ V $$ 일 때, row는 문서를 column은 단어를 가르키는 $$ D \times V $$ 차원의 matrix로 변환시키겠습니다. 가중치는 아래와 같이 사용하겠습니다.

$$ w(t,d,D) = log(tf(t,d)+1) \times log(\frac{N}{df(t,D)} + 1) $$

{% highlight ruby %}
def compute_tfidf(index,word_dictionary,doc_dictionary):
    V = len(word_dictionary)
    D = len(doc_dictionary)
    tf = np.zeros((D,V))            # tf matrix 생성
    for doc in index:               # document별로 반복
        vector = np.zeros(V)    
        for word in index[doc]:
            vector[word_dictionary[word]] = index[doc][word]   # 1차원 배열에 tf(i,d)값을 채워넣기 시작
        vector = np.log(vector+1)                               
        tf[doc_dictionary[doc]] = vector        
    df = np.sum(np.sign(tf),0)             # 단어 별로 몇개의 문서에서 등장했는지 count합니다.
    idf = np.log(D / df + 1)
    tfidf = tf * idf
    return tfidf

tfidf = compute_tfidf(index,word_dictionary,doc_dictionary)
{% endhighlight %}

### Classifier ###

이제 준비는 끝났습니다. 저는 *The New York Times*에서 *entertainment* category의 [기사 하나][newyork-times]를 저장해 두었습니다.

{% highlight ruby %}
def read_input(file_name):
    doc = ''
    file = open(file_name)
    temp = file.readline()
    for line in temp:
        doc += line.lower()
    stopwrds = stopwords.words('english')
    doc = nltk.word_tokenize(doc)
    doc = [token for token in doc if token not in stopwrds]
    return doc

#tokenize된 input_doc을 tfidf형태의 벡터로 바꿔준다
def convert_to_tfidf(tfidf,word_dictionary,input_doc):
    V = len(word_dictionary)
    D = tfidf.shape[0]
    tf = np.zeros(V)

    for word in input_doc:
        if word in word_dictionary:
            tf[word_dictionary[word]] += 1   
    tf = np.log(tf+1)
    df = np.sum(np.sign(tfidf),0)
    idf = np.log(D / df + 1) 
    return tf*idf
{% endhighlight %}

{% highlight ruby %}
input_doc = read_input('input_document.txt')
input_vec = convert_to_tfidf(tfidf, word_dictionary, input_doc)
{% endhighlight %}

해당 기사를 *tf-idf vector*로 변환시켜 줍니다. 이제 마지막입니다. **Cosine Similarity**를 통해 가장 유사한 문서를 찾아 해당 문서의 *category*를 확인하면 됩니다!

{% highlight ruby %}
def cosine_similarity(x,y):
    normalizing_factor_x = np.sqrt(np.sum(np.square(x)))    #x의 절댓값 크기
    normalizing_factor_y = np.sqrt(np.sum(np.square(y)))    #y의 절댓값 크기
    return np.matmul(x,np.transpose(y))/(normalizing_factor_x*normalizing_factor_y)

def classifier(input_vec, tfidf, doc_dictionary):
    max_score = 0
    label = ''
    
    for doc in doc_dictionary:
        score =  cosine_similarity(input_vec, tfidf[doc_dictionary[doc]])
        if score > max_score:
            label = doc
            max_score = score
        print(doc, score)
    return label
{% endhighlight %}

{% highlight ruby %}
classifier(input_vec,tfidf,doc_dictionary)
#->entertainment13
{% endhighlight %}
<br>
짜잔! 고작 60개의 문서를 썼음에도 불구하고 *entertainment* category로 훌륭하게 분류에 성공했습니다. 

## 마치며

이번 포스팅은 블로그를 첫번째로 개설 후 쓰는 포스팅이라 시간도 많이 걸렸고, 나름대로 뿌듯함이 있는 것 같습니다. **Python Implementation**에서 확인해보셨듯이 **TF-IDF Model**이 작은 데이터만으로도 효과적으로 분류에 성공했습니다. 그럼에도 불구하고, 사실 **TF-IDF Model**만으로는 상업적으로 훌륭한 성능을 보여주지는 못합니다. 하지만, 다른 알고리즘과의 결합, 변형을 통해서 충분히 활용되고 있는 알고리즘입니다. **TF-IDF Model**은 뉴스의 장르를 분류하는데 사용될 뿐만이 아니라, 여러분의 뉴스 검색에 있어서도 뉴수의 순위를 결정하는데 있어서도 핵심적인 역할을 하고 있는 알고리즘입니다. 궁금하신점 있으시면, 댓글 남겨주세요!



[newyork-times]: https://www.nytimes.com/2018/01/28/arts/music/grammy-awards.html
[tf_github]: https://github.com/joonleesky/tfidf-tutorial

<div id="disqus_thread"></div>
<script>

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://https-joonleesky-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
                            
